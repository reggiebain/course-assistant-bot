{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9879418,"sourceType":"datasetVersion","datasetId":5994754}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Pipeline\n#### Steps\n1. Import, process, and split document base\n2. Load, instantiate, and apply embedding model\n3. Create vector store with index\n4. Prompt the model and feed context.\n5. Create and implement retreiver (with optional re ranking)\n6. Parse responses","metadata":{}},{"cell_type":"code","source":"!pip install -q torch transformers transformers sentence-transformers tqdm openpyxl pandas datasets ragatouille\n!pip install -q langchain langchain-huggingface langchain-community langchain-huggingface langchain-openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:28:28.193892Z","iopub.execute_input":"2024-11-23T19:28:28.194317Z","iopub.status.idle":"2024-11-23T19:29:14.312441Z","shell.execute_reply.started":"2024-11-23T19:28:28.194253Z","shell.execute_reply":"2024-11-23T19:29:14.310764Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Optional, List, Tuple\nimport json\nimport datasets\nimport os\nimport re\nimport torch\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom transformers import AutoTokenizer\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nfrom langchain_community.llms import HuggingFaceHub\nfrom langchain_huggingface import HuggingFaceEndpoint\nfrom ragatouille import RAGPretrainedModel\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain_core.language_models.llms import LLM\nfrom langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\nfrom langchain.schema import Document\nfrom langchain_core.language_models import BaseChatModel\n\nfrom sklearn.model_selection import ParameterGrid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:06:15.406624Z","iopub.execute_input":"2024-11-23T20:06:15.407055Z","iopub.status.idle":"2024-11-23T20:06:15.416485Z","shell.execute_reply.started":"2024-11-23T20:06:15.407020Z","shell.execute_reply":"2024-11-23T20:06:15.415203Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:30:26.734248Z","iopub.execute_input":"2024-11-23T19:30:26.734725Z","iopub.status.idle":"2024-11-23T19:30:26.815158Z","shell.execute_reply.started":"2024-11-23T19:30:26.734688Z","shell.execute_reply":"2024-11-23T19:30:26.813636Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_documents(file_path):\n    _, file_extension = os.path.splitext(file_path)\n\n    if file_extension.lower() == '.pdf':\n        loader = PyPDFLoader(file_path)\n        print(\"Loading PDF document...\")\n    elif file_extension.lower() == '.md':\n        loader = UnstructuredMarkdownLoader(file_path)\n        print(\"Loading Markdown document...\")\n    elif file_extension.lower() == '.html':\n        loader = UnstructuredHTMLLoader(file_path)\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n    \n    documents = loader.load()\n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:32:00.984968Z","iopub.execute_input":"2024-11-23T19:32:00.985441Z","iopub.status.idle":"2024-11-23T19:32:00.992468Z","shell.execute_reply.started":"2024-11-23T19:32:00.985386Z","shell.execute_reply":"2024-11-23T19:32:00.991148Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Implement splitting of docs via text_splitter\ndef split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:30:27.778118Z","iopub.execute_input":"2024-11-23T19:30:27.778530Z","iopub.status.idle":"2024-11-23T19:30:27.786339Z","shell.execute_reply.started":"2024-11-23T19:30:27.778496Z","shell.execute_reply":"2024-11-23T19:30:27.785023Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Method to load embeddins and create vectore store\ndef load_embeddings(langchain_docs, chunk_size, embedding_model_name: Optional[str] = \"thenlper/gte-small\",) -> FAISS:\n\n    embedding_model = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        multi_process=True,\n        #model_kwargs={\"device\": \"cuda\"},\n        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n    )\n\n    # Check if embeddings already exist on disk\n    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n    index_folder_path = f\"./data/indexes/{index_name}/\"\n    if os.path.isdir(index_folder_path):\n        return FAISS.load_local(\n            index_folder_path,\n            embedding_model,\n            distance_strategy=DistanceStrategy.COSINE,\n            allow_dangerous_deserialization=True\n        )\n\n    else:\n        print(\"Index not found, generating it...\")\n        docs_processed = split_documents(\n            chunk_size,\n            langchain_docs,\n            embedding_model_name,\n        )\n        knowledge_index = FAISS.from_documents(\n            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE,\n            #allow_dangerous_deserialization=True\n        )\n        knowledge_index.save_local(index_folder_path)\n        return knowledge_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:08.271443Z","iopub.execute_input":"2024-11-23T20:05:08.271858Z","iopub.status.idle":"2024-11-23T20:05:08.279892Z","shell.execute_reply.started":"2024-11-23T20:05:08.271824Z","shell.execute_reply":"2024-11-23T20:05:08.278475Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"\n<|system|>\nUsing the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\n</s>\n<|assistant|>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:09.798252Z","iopub.execute_input":"2024-11-23T20:05:09.799548Z","iopub.status.idle":"2024-11-23T20:05:09.804491Z","shell.execute_reply.started":"2024-11-23T20:05:09.799493Z","shell.execute_reply":"2024-11-23T20:05:09.803112Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Zephyr 7b from Mistral AI\nrepo_id = \"HuggingFaceH4/zephyr-7b-beta\"\nREADER_MODEL_NAME = \"zephyr-7b-beta\"\n\n# LLama 3.1 8B Instruct from Meta\n#repo_id = 'meta-llama/Llama-3.1-8B-Instruct'\n#READER_MODEL_NAME = 'Llama-3.1-8B-Instruct'\n\n# Declare some parameters for the RAG Q&A LLM\nreader_model_params = {\"max_new_tokens\": 512, \"top_k\": 30,\"temperature\": 0.1,\"repetition_penalty\": 1.03,}\n\nreader_llm = HuggingFaceHub(\n    repo_id=repo_id,\n    task=\"text-generation\", \n    huggingfacehub_api_token=HF_TOKEN,\n    model_kwargs = reader_model_params,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:10.832044Z","iopub.execute_input":"2024-11-23T20:05:10.832513Z","iopub.status.idle":"2024-11-23T20:05:10.839380Z","shell.execute_reply.started":"2024-11-23T20:05:10.832477Z","shell.execute_reply":"2024-11-23T20:05:10.837467Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def answer_with_rag(question: str, llm: LLM, knowledge_index: VectorStore, \n                    reranker: Optional[RAGPretrainedModel] = None, num_retrieved_docs: int = 30,\n                    num_docs_final: int = 7,) -> Tuple[str, List[Document]]:\n    \n    # Gather documents with retriever\n    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n\n    # Optionally rerank results using RAGatoulli ColBERT model\n    if reranker:\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n    relevant_docs = relevant_docs[:num_docs_final]\n\n    # Build the final prompt\n    context = \"\\nExtracted documents:\\n\"\n    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n\n    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n\n    # Redact an answer\n    answer = llm(final_prompt)\n    #answer = llm.invoke(final_prompt)\n\n    return answer, relevant_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:16.365658Z","iopub.execute_input":"2024-11-23T20:05:16.366240Z","iopub.status.idle":"2024-11-23T20:05:16.374700Z","shell.execute_reply.started":"2024-11-23T20:05:16.366197Z","shell.execute_reply":"2024-11-23T20:05:16.373567Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Method to run rag on set of questions\ndef run_rag_tests(eval_dataset, llm, knowledge_index, output_file,\n                  reranker: Optional[RAGPretrainedModel] = None, verbose: Optional[bool] = True,\n                  test_settings: Optional[str] = None,):\n    \n    try:  # load previous generations if they exist\n        with open(output_file, \"r\") as f:\n            outputs = json.load(f)\n    except:\n        outputs = []\n\n    # loop over the q&a pairs in the eval dataset \n    for example in tqdm(eval_dataset):\n        question = example[\"question\"]\n        if question in [output[\"question\"] for output in outputs]:\n            continue\n\n        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n        if verbose:\n            print(\"=======================================================\")\n            print(f\"Question: {question}\")\n            print(f\"Answer: {answer}\")\n            print(f'True answer: {example[\"answer\"]}')\n        result = {\n            \"question\": question,\n            \"true_answer\": example[\"answer\"],\n            \"source_doc\": example[\"source_doc\"],\n            \"generated_answer\": answer,\n            \"retrieved_docs\": [doc for doc in relevant_docs],\n        }\n        # Optionally include the details about the model settings throughout the pipeline\n        if test_settings:\n            result[\"test_settings\"] = test_settings\n        outputs.append(result)\n\n        # Print everything to an output json file\n        with open(output_file, \"w\") as f:\n            json.dump(outputs, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:17.945975Z","iopub.execute_input":"2024-11-23T20:05:17.946404Z","iopub.status.idle":"2024-11-23T20:05:17.956195Z","shell.execute_reply.started":"2024-11-23T20:05:17.946369Z","shell.execute_reply":"2024-11-23T20:05:17.954921Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"RAW_KNOWLEDGE_BASE = load_documents('/kaggle/input/course-bot-data/documents/rbain_syllabus.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:19.260123Z","iopub.execute_input":"2024-11-23T20:05:19.260520Z","iopub.status.idle":"2024-11-23T20:05:19.923651Z","shell.execute_reply.started":"2024-11-23T20:05:19.260487Z","shell.execute_reply":"2024-11-23T20:05:19.922279Z"}},"outputs":[{"name":"stdout","text":"Loading PDF document...\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceHub\nfrom langchain_huggingface import HuggingFaceEndpoint\n\n# Zephyr 7b from Mistral AI\nrepo_id = \"HuggingFaceH4/zephyr-7b-beta\"\nREADER_MODEL_NAME = \"zephyr-7b-beta\"\n\n# LLama 3.1 8B Instruct from Meta\n#repo_id = 'meta-llama/Llama-3.1-8B-Instruct'\n#READER_MODEL_NAME = 'Llama-3.1-8B-Instruct'\n\n# Declare some parameters for the RAG Q&A LLM\nreader_model_params = {\"max_new_tokens\": 512, \"top_k\": 30,\"temperature\": 0.1,\"repetition_penalty\": 1.03,}\n\nreader_llm = HuggingFaceHub(\n    repo_id=repo_id,\n    task=\"text-generation\", \n    huggingfacehub_api_token=HF_TOKEN,\n    model_kwargs = reader_model_params,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:20.971548Z","iopub.execute_input":"2024-11-23T20:05:20.971953Z","iopub.status.idle":"2024-11-23T20:05:20.978744Z","shell.execute_reply.started":"2024-11-23T20:05:20.971919Z","shell.execute_reply":"2024-11-23T20:05:20.977545Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Prepare data set of questions\nquestions = pd.read_json('/kaggle/input/course-bot-data/documents/rbain_syllabus_questions.json')\nquestion = questions.iloc[0].question\nquestion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:05:24.277458Z","iopub.execute_input":"2024-11-23T20:05:24.277843Z","iopub.status.idle":"2024-11-23T20:05:24.291082Z","shell.execute_reply.started":"2024-11-23T20:05:24.277812Z","shell.execute_reply":"2024-11-23T20:05:24.289812Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'What topics are included in this course?'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"'''\nhyperparameters = {\n    \"chunk_size\": [200],  # Add other chunk sizes as needed\n    \"embeddings\": [\"thenlper/gte-small\"],  # Add other embeddings as needed\n    \"rerank\": [True, False],\n}\n\n# Generate all combinations of hyperparameters\nfor params in ParameterGrid(hyperparameters):\n    chunk_size = params[\"chunk_size\"]\n    embeddings = params[\"embeddings\"]\n    rerank = params[\"rerank\"]\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:29:27.324898Z","iopub.status.idle":"2024-11-23T19:29:27.325671Z","shell.execute_reply.started":"2024-11-23T19:29:27.325386Z","shell.execute_reply":"2024-11-23T19:29:27.325433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_question_answer(rag_output):\n    # Split the output into lines for easier parsing\n    lines = rag_output.splitlines()\n\n    # Initialize variables\n    question = None\n    answer = None\n\n    # Look for the question and answer markers\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"Question:\"):\n            # Extract the question text\n            question = line.replace(\"Question:\", \"\").strip()\n        elif line.startswith(\"<|assistant|>\"):\n            # Answer follows <|assistant|>, so join subsequent lines\n            answer_start_index = lines.index(line) + 1\n            answer = \" \".join(l.strip() for l in lines[answer_start_index:])\n            break\n\n    # Return the parsed question and answer\n    #return {\"question\": question, \"answer\": answer}\n    return question","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:24:55.959056Z","iopub.execute_input":"2024-11-23T20:24:55.959580Z","iopub.status.idle":"2024-11-23T20:24:55.967433Z","shell.execute_reply.started":"2024-11-23T20:24:55.959541Z","shell.execute_reply":"2024-11-23T20:24:55.966126Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"print(parse_question_answer(answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:11:38.442132Z","iopub.execute_input":"2024-11-23T20:11:38.442586Z","iopub.status.idle":"2024-11-23T20:11:38.448890Z","shell.execute_reply.started":"2024-11-23T20:11:38.442548Z","shell.execute_reply":"2024-11-23T20:11:38.447466Z"}},"outputs":[{"name":"stdout","text":"{'question': 'What topics are included in this course?', 'answer': \"The topics included in this course are motion in one and two dimensions, Newton's laws of motion, equilibrium, work, energy, momentum, rotational motion, gravity, heat, and waves, with examples from medicine and biology where possible. This information can be found in Document 1, line 16-26.\"}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def process_questions_with_answers(json_file_path, output_file_path, llm, knowledge_index):\n\n    with open(json_file_path, \"r\") as file:\n        questions = json.load(file)\n\n    for question_obj in questions:\n        question = question_obj.get(\"question\", \"\")\n        if question:  # Ensure there is a valid question\n            answer =  answer_with_rag(question, llm, knowledge_index)[0]\n            question_obj[\"answer\"] = parse_question_answer(answer)\n\n    with open(output_file_path, \"w\") as file:\n        json.dump(questions, file, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:28:43.264186Z","iopub.execute_input":"2024-11-23T20:28:43.264653Z","iopub.status.idle":"2024-11-23T20:28:43.271847Z","shell.execute_reply.started":"2024-11-23T20:28:43.264614Z","shell.execute_reply":"2024-11-23T20:28:43.270655Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"if not os.path.exists(\"./output\"):\n    os.mkdir(\"./output\")\n\nhyperparameters = {\n    \"chunk_size\": [200],  # Add other chunk sizes as needed\n    \"embeddings\": [\"thenlper/gte-small\"],  # Add other embeddings as needed\n    \"rerank\": [True, False],\n}\n\n# Generate all combinations of hyperparameters\nfor params in ParameterGrid(hyperparameters):\n    chunk_size = params[\"chunk_size\"]\n    embeddings = params[\"embeddings\"]\n    rerank = params[\"rerank\"]\n\n\n#for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n#    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n#        for rerank in [True, False]:\n    settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n    output_file_name = f\"./output/rag_{settings_name}.json\"\n\n    print(f\"Running evaluation for {settings_name}:\")\n\n    print(\"Creating embeddings...\")\n    knowledge_index = load_embeddings(\n        RAW_KNOWLEDGE_BASE,\n        chunk_size=chunk_size,\n        embedding_model_name=embeddings,\n    )\n\n    print(\"Implementing RAG...\")\n    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n    \n    # Example usage\n    input_file = \"/kaggle/input/course-bot-data/documents/rbain_syllabus_questions.json\"  # Replace with your input file path\n    output_file = \"questions_with_answers.json\"  # Replace with your output file path\n\n    process_questions_with_answers(input_file, output_file, reader_llm, knowledge_index)\n    #answer_with_rag(question, reader_llm, knowledge_index)\n    #run_rag_tests(\n    #    eval_dataset=eval_dataset,\n    #    llm=reader_llm,\n    #    knowledge_index=knowledge_index,\n    #    output_file=output_file_name,\n    #    reranker=reranker,\n    #    verbose=False,\n    #    test_settings=settings_name,\n    #)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:28:43.729148Z","iopub.execute_input":"2024-11-23T20:28:43.730283Z","iopub.status.idle":"2024-11-23T20:40:24.941026Z","shell.execute_reply.started":"2024-11-23T20:28:43.730238Z","shell.execute_reply":"2024-11-23T20:40:24.939781Z"}},"outputs":[{"name":"stdout","text":"Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta:\nCreating embeddings...\nImplementing RAG...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\nCreating embeddings...\nImplementing RAG...\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}