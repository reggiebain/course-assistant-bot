{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to use the venv generated via `make` then make sure to select that kernel via Jupyter\n",
    "#   create venv + install requirements.txt via `make`\n",
    "#   clean via `make clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:36:05.168073Z",
     "iopub.status.busy": "2024-11-17T19:36:05.167643Z",
     "iopub.status.idle": "2024-11-17T19:36:05.251619Z",
     "shell.execute_reply": "2024-11-17T19:36:05.250428Z",
     "shell.execute_reply.started": "2024-11-17T19:36:05.168030Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:36:05.253517Z",
     "iopub.status.busy": "2024-11-17T19:36:05.253133Z",
     "iopub.status.idle": "2024-11-17T19:36:05.324072Z",
     "shell.execute_reply": "2024-11-17T19:36:05.322801Z",
     "shell.execute_reply.started": "2024-11-17T19:36:05.253437Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:44:53.516980Z",
     "iopub.status.busy": "2024-11-17T19:44:53.516558Z",
     "iopub.status.idle": "2024-11-17T19:44:53.674349Z",
     "shell.execute_reply": "2024-11-17T19:44:53.673179Z",
     "shell.execute_reply.started": "2024-11-17T19:44:53.516941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load variables from .env file into the environment\n",
    "# Access the environment variables\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# TODO: note, it looks like huggingface might auto detect the .env in which case this isn't necessary\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:44:56.568053Z",
     "iopub.status.busy": "2024-11-17T19:44:56.567592Z",
     "iopub.status.idle": "2024-11-17T19:44:56.644255Z",
     "shell.execute_reply": "2024-11-17T19:44:56.642999Z",
     "shell.execute_reply.started": "2024-11-17T19:44:56.568006Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n",
    "#from langchain.docstore.document import Document\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Function to clean text (to remove unwanted line breaks within sentences)\n",
    "def clean_text(text):\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "# Function to load documents based on file type\n",
    "def load_documents(file_path):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        print(\"Loading PDF document...\")\n",
    "    elif file_extension.lower() == '.md':\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        print(\"Loading Markdown document...\")\n",
    "    elif file_extension.lower() == '.html':\n",
    "        loader = UnstructuredHTMLLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:57:12.494588Z",
     "iopub.status.busy": "2024-11-17T19:57:12.494091Z",
     "iopub.status.idle": "2024-11-17T19:57:13.227756Z",
     "shell.execute_reply": "2024-11-17T19:57:13.226605Z",
     "shell.execute_reply.started": "2024-11-17T19:57:12.494539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF document...\n"
     ]
    }
   ],
   "source": [
    "# Load the document and questions\n",
    "file_path = \"../documents/rbain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\n",
    "documents = load_documents(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:57:38.985184Z",
     "iopub.status.busy": "2024-11-17T19:57:38.984691Z",
     "iopub.status.idle": "2024-11-17T19:57:39.088178Z",
     "shell.execute_reply": "2024-11-17T19:57:39.086799Z",
     "shell.execute_reply.started": "2024-11-17T19:57:38.985107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 13\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set some params\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "docs_processed = []\n",
    "for doc in documents:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"Total chunks created: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:03:01.987203Z",
     "iopub.status.busy": "2024-11-17T20:03:01.986738Z",
     "iopub.status.idle": "2024-11-17T20:03:02.059305Z",
     "shell.execute_reply": "2024-11-17T20:03:02.058089Z",
     "shell.execute_reply.started": "2024-11-17T20:03:01.987157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I.\\nCourse\\nDescription\\n1.\\nCourse\\nSummary\\na.\\nPHY\\n161/PHYS\\n215\\nGeneral\\nPhysics\\nI\\nis\\nan\\nalgebra-based\\nintroduction\\nto\\nmechanics,\\nthermodynamics,\\nand\\nwaves.\\nTopics\\ninclude\\nmotion\\nin\\none\\nand\\ntwo\\ndimensions,\\nNewton’s\\nlaws\\nof\\nmotion,\\nequilibrium,\\nwork,\\nenergy,\\nmomentum,\\nrotational\\nmotion,\\ngravity,\\nheat,\\nwaves,\\nand\\nsound.\\nExamples\\nfrom\\nmedicine\\nand\\nbiology\\nwill\\nbe\\nincluded\\nwhenever\\npossible.\\n2.\\nCollege\\nCredit\\nHours\\n(Dual-Enrollment)\\na.\\nThis\\ncourse\\nis\\ndual\\nenrolled\\nwith\\nPHYS\\n215\\nGeneral\\nPhysics\\nI\\nat\\nFrancis\\nMarion\\nUniversity\\n(FMU)\\nand\\ntaught\\nby\\na\\nGSSM\\ninstructor.\\nStudents\\nwill\\neach\\nhave\\na\\nFMU\\ntranscript\\nwith\\ntheir\\noverall\\ngrade\\nearned\\nin\\nthis\\ncourse.\\nStudents\\nmay\\nearn\\nup\\nto\\n4\\ncollege\\ncredit\\nhours\\ndepending\\non\\ntheir\\ngrade\\nand\\nthe\\ntransfer\\npolicies\\nof\\ntheir\\ncollege/university.\\nRefer\\nto\\nthe\\nDual \\nEnrollment \\nFAQ \\nin \\nthe \\nCourse \\nCatalog \\nfor\\nmore\\ninformation.\\n3.\\nLearning\\nOutcomes\\na.\\nUpon\\ncompletion\\nof\\nthis\\ncourse,\\nstudents\\nwill\\nbe\\nable\\nto:\\nb.\\nApply\\nthe\\nlaws\\nof\\nclassical\\nNewtonian\\nmechanics\\n(motion,\\nforce,\\nenergy,\\nmomentum,\\nand\\ngravitation)\\nto\\nsolve\\nproblems\\ninvolving\\nstatic\\nsystems,\\nmotion\\nwith\\nconstant\\nacceleration,\\nand\\nrotational\\nmotion.\\nc.\\nUse\\ntechniques\\nof\\ngraphical\\nanalysis\\nto\\nclassify\\nand\\nmodel\\nphysical\\nphenomena\\nand\\napply\\nphysical\\nmeaning\\nto\\nmathematical\\ntechniques\\nfrom\\nalgebra,\\npre-calculus,\\nand\\ntrigonometry.\\nPage\\n1\\nof\\n8\\nCourse\\nPHY\\n161\\n-\\nGeneral\\nPhysics\\n1\\n(Dual\\nEnrolled\\nw/\\nFMU)\\nSemester\\nFall\\n2024\\nLecture\\nTuThFr\\n1-2\\n|\\nTuThFr\\n2-3\\n|\\nTuThFr\\n3-4\\nLab\\nMo\\n2-3\\n|\\nTu\\n10-12\\n|\\nWe\\n2-4\\nLocation\\nC-107\\nand\\nhttps://gssm.zoom.us/j/8622923374\\nInstructor\\nDr.\\nReginald\\nBain\\n| ✉\\nrbain@governors.school\\n📞\\n843-383-3900\\n| 🏢\\nOﬃce:\\nB166/Zoom\\n(862-292-3374)\\n🕐\\nOﬃce\\nHours:\\nBy\\nappointment\\nOR\\nMoWe\\n10-11,\\n1-2,\\nThFr\\n10-11'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:57:47.200160Z",
     "iopub.status.busy": "2024-11-17T19:57:47.199709Z",
     "iopub.status.idle": "2024-11-17T19:57:47.355803Z",
     "shell.execute_reply": "2024-11-17T19:57:47.354632Z",
     "shell.execute_reply.started": "2024-11-17T19:57:47.200103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:57:49.938935Z",
     "iopub.status.busy": "2024-11-17T19:57:49.938461Z",
     "iopub.status.idle": "2024-11-17T19:57:50.009972Z",
     "shell.execute_reply": "2024-11-17T19:57:50.008648Z",
     "shell.execute_reply.started": "2024-11-17T19:57:49.938886Z"
    }
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:57:51.981664Z",
     "iopub.status.busy": "2024-11-17T19:57:51.981095Z",
     "iopub.status.idle": "2024-11-17T20:00:23.038743Z",
     "shell.execute_reply": "2024-11-17T20:00:23.037215Z",
     "shell.execute_reply.started": "2024-11-17T19:57:51.981614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b996f8feb24b6fa7158376f3978c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    time.sleep(1)\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:00:23.077829Z",
     "iopub.status.busy": "2024-11-17T20:00:23.077474Z",
     "iopub.status.idle": "2024-11-17T20:00:23.155602Z",
     "shell.execute_reply": "2024-11-17T20:00:23.154227Z",
     "shell.execute_reply.started": "2024-11-17T20:00:23.077790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in\\ncertain\\ncases.\\nd.\\nNo \\nHeadphones: \\nSt...</td>\n",
       "      <td>What is the policy on using headphones during ...</td>\n",
       "      <td>Wearing headphones during class is prohibited.</td>\n",
       "      <td>../documents/rbain_syllabus.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a.\\nStudents\\nwill\\nsubmit\\nHW\\nassignments\\na...</td>\n",
       "      <td>What is the percentage of the final exam grade?\\n</td>\n",
       "      <td>15%</td>\n",
       "      <td>../documents/rbain_syllabus.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>October\\n1st\\n3.\\nExam\\n3\\n(Ch\\n4/5-6)\\n—\\nTue...</td>\n",
       "      <td>What chapters are covered in the third lab?\\n</td>\n",
       "      <td>The third lab covers 1D kinematics.</td>\n",
       "      <td>../documents/rbain_syllabus.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d.\\nWork\\ncollaboratively\\ninside\\nand\\noutsid...</td>\n",
       "      <td>What is the primary textbook for the course?\\n</td>\n",
       "      <td>The primary textbook for the course is Physics...</td>\n",
       "      <td>../documents/rbain_syllabus.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I.\\nCourse\\nDescription\\n1.\\nCourse\\nSummary\\n...</td>\n",
       "      <td>How many college credit hours can students ear...</td>\n",
       "      <td>Students can earn up to 4 college credit hours...</td>\n",
       "      <td>../documents/rbain_syllabus.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  in\\ncertain\\ncases.\\nd.\\nNo \\nHeadphones: \\nSt...   \n",
       "1  a.\\nStudents\\nwill\\nsubmit\\nHW\\nassignments\\na...   \n",
       "2  October\\n1st\\n3.\\nExam\\n3\\n(Ch\\n4/5-6)\\n—\\nTue...   \n",
       "3  d.\\nWork\\ncollaboratively\\ninside\\nand\\noutsid...   \n",
       "4  I.\\nCourse\\nDescription\\n1.\\nCourse\\nSummary\\n...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the policy on using headphones during ...   \n",
       "1  What is the percentage of the final exam grade?\\n   \n",
       "2      What chapters are covered in the third lab?\\n   \n",
       "3     What is the primary textbook for the course?\\n   \n",
       "4  How many college credit hours can students ear...   \n",
       "\n",
       "                                              answer  \\\n",
       "0     Wearing headphones during class is prohibited.   \n",
       "1                                                15%   \n",
       "2                The third lab covers 1D kinematics.   \n",
       "3  The primary textbook for the course is Physics...   \n",
       "4  Students can earn up to 4 college credit hours...   \n",
       "\n",
       "                        source_doc  \n",
       "0  ../documents/rbain_syllabus.pdf  \n",
       "1  ../documents/rbain_syllabus.pdf  \n",
       "2  ../documents/rbain_syllabus.pdf  \n",
       "3  ../documents/rbain_syllabus.pdf  \n",
       "4  ../documents/rbain_syllabus.pdf  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_df = pd.DataFrame(outputs)\n",
    "outputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:46:12.221205Z",
     "iopub.status.busy": "2024-11-17T19:46:12.219849Z",
     "iopub.status.idle": "2024-11-17T19:46:12.293339Z",
     "shell.execute_reply": "2024-11-17T19:46:12.291903Z",
     "shell.execute_reply.started": "2024-11-17T19:46:12.221142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context', 'question', 'answer', 'source_doc'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:46:12.295786Z",
     "iopub.status.busy": "2024-11-17T19:46:12.295248Z",
     "iopub.status.idle": "2024-11-17T19:46:12.368448Z",
     "shell.execute_reply": "2024-11-17T19:46:12.366920Z",
     "shell.execute_reply.started": "2024-11-17T19:46:12.295730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the policy on using headphones during class?\n",
      "\n",
      "Answer: Wearing headphones during class is prohibited.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {outputs_df.iloc[0,:]['question']}\")\n",
    "print(f\"Answer: {outputs_df.iloc[0,:]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:46:12.370375Z",
     "iopub.status.busy": "2024-11-17T19:46:12.369958Z",
     "iopub.status.idle": "2024-11-17T19:46:12.447903Z",
     "shell.execute_reply": "2024-11-17T19:46:12.446476Z",
     "shell.execute_reply.started": "2024-11-17T19:46:12.370332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the policy on using headphones during ...</td>\n",
       "      <td>Wearing headphones during class is prohibited.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the percentage of the final exam grade?\\n</td>\n",
       "      <td>15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What chapters are covered in the third lab?\\n</td>\n",
       "      <td>The third lab covers 1D kinematics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the primary textbook for the course?\\n</td>\n",
       "      <td>The primary textbook for the course is Physics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many college credit hours can students ear...</td>\n",
       "      <td>Students can earn up to 4 college credit hours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the hours of the Center for Academic ...</td>\n",
       "      <td>The Center for Academic Success is open from 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What should students bring to the exam?\\n</td>\n",
       "      <td>Students should bring a writing utensil, a lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the fundamental values that students ...</td>\n",
       "      <td>The fundamental values that students are expec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the policy on using headphones during ...   \n",
       "1  What is the percentage of the final exam grade?\\n   \n",
       "2      What chapters are covered in the third lab?\\n   \n",
       "3     What is the primary textbook for the course?\\n   \n",
       "4  How many college credit hours can students ear...   \n",
       "5  What are the hours of the Center for Academic ...   \n",
       "6          What should students bring to the exam?\\n   \n",
       "7  What are the fundamental values that students ...   \n",
       "\n",
       "                                              answer  \n",
       "0     Wearing headphones during class is prohibited.  \n",
       "1                                                15%  \n",
       "2                The third lab covers 1D kinematics.  \n",
       "3  The primary textbook for the course is Physics...  \n",
       "4  Students can earn up to 4 college credit hours...  \n",
       "5  The Center for Academic Success is open from 8...  \n",
       "6  Students should bring a writing utensil, a lap...  \n",
       "7  The fundamental values that students are expec...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_df[['question', 'answer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Groundedness: can the question be answered from the given context?\n",
    "- Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n",
    "- Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n",
    "- Faithfulness: number of claims in the generated answer that can be inferred from given context / total number of claims in generated answer\n",
    "-   Set of claims from generated answer identified\n",
    "-   Each claim cross checked within the context.\n",
    "- https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:46:20.871574Z",
     "iopub.status.busy": "2024-11-17T19:46:20.871157Z",
     "iopub.status.idle": "2024-11-17T19:46:20.943897Z",
     "shell.execute_reply": "2024-11-17T19:46:20.942723Z",
     "shell.execute_reply.started": "2024-11-17T19:46:20.871536Z"
    }
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:46:46.755708Z",
     "iopub.status.busy": "2024-11-17T19:46:46.755210Z",
     "iopub.status.idle": "2024-11-17T19:48:10.193088Z",
     "shell.execute_reply": "2024-11-17T19:48:10.191891Z",
     "shell.execute_reply.started": "2024-11-17T19:46:46.755665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b58953c11b4bbba3b5a7bfcea9e625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T19:48:10.232926Z",
     "iopub.status.busy": "2024-11-17T19:48:10.232576Z",
     "iopub.status.idle": "2024-11-17T19:48:10.391893Z",
     "shell.execute_reply": "2024-11-17T19:48:10.390200Z",
     "shell.execute_reply.started": "2024-11-17T19:48:10.232887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the policy on using headphones during class?\\n</td>\n",
       "      <td>Wearing headphones during class is prohibited.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the percentage of the final exam grade?\\n</td>\n",
       "      <td>15%</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What chapters are covered in the third lab?\\n</td>\n",
       "      <td>The third lab covers 1D kinematics.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the primary textbook for the course?\\n</td>\n",
       "      <td>The primary textbook for the course is Physics: Principles with Applications, 7th ed., Douglas C. Giancoli.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many college credit hours can students earn in PHY 161?\\n</td>\n",
       "      <td>Students can earn up to 4 college credit hours depending on their grade and the transfer policies of their college/university.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the hours of the Center for Academic Success?\\n</td>\n",
       "      <td>The Center for Academic Success is open from 8-10 pm Sunday-Thursday during the school year. However, hours may differ when labs are virtual. No appointments are necessary.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What should students bring to the exam?\\n</td>\n",
       "      <td>Students should bring a writing utensil, a laptop/tablet for entering answers on WebAssign and a phone to scan/upload any written work for free response questions. They should also bring an AP approved calculator. A formula sheet and scratch paper will be provided.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the fundamental values that students are expected to uphold in the class?\\n</td>\n",
       "      <td>The fundamental values that students are expected to uphold in the class are honesty, trust, fairness, respect, responsibility, and courage.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               question  \\\n",
       "0                                What is the policy on using headphones during class?\\n   \n",
       "1                                     What is the percentage of the final exam grade?\\n   \n",
       "2                                         What chapters are covered in the third lab?\\n   \n",
       "3                                        What is the primary textbook for the course?\\n   \n",
       "4                         How many college credit hours can students earn in PHY 161?\\n   \n",
       "5                              What are the hours of the Center for Academic Success?\\n   \n",
       "6                                             What should students bring to the exam?\\n   \n",
       "7  What are the fundamental values that students are expected to uphold in the class?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                      answer  \\\n",
       "0                                                                                                                                                                                                                             Wearing headphones during class is prohibited.   \n",
       "1                                                                                                                                                                                                                                                                        15%   \n",
       "2                                                                                                                                                                                                                                        The third lab covers 1D kinematics.   \n",
       "3                                                                                                                                                                The primary textbook for the course is Physics: Principles with Applications, 7th ed., Douglas C. Giancoli.   \n",
       "4                                                                                                                                             Students can earn up to 4 college credit hours depending on their grade and the transfer policies of their college/university.   \n",
       "5                                                                                               The Center for Academic Success is open from 8-10 pm Sunday-Thursday during the school year. However, hours may differ when labs are virtual. No appointments are necessary.   \n",
       "6  Students should bring a writing utensil, a laptop/tablet for entering answers on WebAssign and a phone to scan/upload any written work for free response questions. They should also bring an AP approved calculator. A formula sheet and scratch paper will be provided.   \n",
       "7                                                                                                                               The fundamental values that students are expected to uphold in the class are honesty, trust, fairness, respect, responsibility, and courage.   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                 5.0              1.0               5.0  \n",
       "1                 2.0              1.0               5.0  \n",
       "2                 1.0              1.0               5.0  \n",
       "3                 5.0              4.0               5.0  \n",
       "4                 NaN              NaN               NaN  \n",
       "5                 2.0              1.0               5.0  \n",
       "6                 5.0              1.0               5.0  \n",
       "7                 5.0              1.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the primary textbook for the course?\\n</td>\n",
       "      <td>The primary textbook for the course is Physics: Principles with Applications, 7th ed., Douglas C. Giancoli.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question  \\\n",
       "3  What is the primary textbook for the course?\\n   \n",
       "\n",
       "                                                                                                        answer  \\\n",
       "3  The primary textbook for the course is Physics: Principles with Applications, 7th ed., Douglas C. Giancoli.   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "3                 5.0              4.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Method for printing the kind of messy outputs from the requests above\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:46:40.488098Z",
     "iopub.status.idle": "2024-11-17T19:46:40.488639Z",
     "shell.execute_reply": "2024-11-17T19:46:40.488397Z",
     "shell.execute_reply.started": "2024-11-17T19:46:40.488369Z"
    }
   },
   "outputs": [],
   "source": [
    "#eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:46:40.490945Z",
     "iopub.status.idle": "2024-11-17T19:46:40.492286Z",
     "shell.execute_reply": "2024-11-17T19:46:40.491971Z",
     "shell.execute_reply.started": "2024-11-17T19:46:40.491936Z"
    }
   },
   "outputs": [],
   "source": [
    "#eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:04:32.860546Z",
     "iopub.status.busy": "2024-11-17T20:04:32.860035Z",
     "iopub.status.idle": "2024-11-17T20:04:37.410570Z",
     "shell.execute_reply": "2024-11-17T20:04:37.409325Z",
     "shell.execute_reply.started": "2024-11-17T20:04:32.860501Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.095867Z",
     "iopub.status.idle": "2024-11-17T19:39:45.096504Z",
     "shell.execute_reply": "2024-11-17T19:39:45.096197Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.096166Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[Document],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.098993Z",
     "iopub.status.idle": "2024-11-17T19:39:45.099468Z",
     "shell.execute_reply": "2024-11-17T19:39:45.099274Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.099250Z"
    }
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.100843Z",
     "iopub.status.idle": "2024-11-17T19:39:45.101321Z",
     "shell.execute_reply": "2024-11-17T19:39:45.101091Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.101069Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "HF_API_TOKEN = HF_TOKEN\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.103132Z",
     "iopub.status.idle": "2024-11-17T19:39:45.103623Z",
     "shell.execute_reply": "2024-11-17T19:39:45.103411Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.103388Z"
    }
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.105505Z",
     "iopub.status.idle": "2024-11-17T19:39:45.105979Z",
     "shell.execute_reply": "2024-11-17T19:39:45.105776Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.105753Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.108143Z",
     "iopub.status.idle": "2024-11-17T19:39:45.108615Z",
     "shell.execute_reply": "2024-11-17T19:39:45.108419Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.108397Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.110563Z",
     "iopub.status.idle": "2024-11-17T19:39:45.111334Z",
     "shell.execute_reply": "2024-11-17T19:39:45.111083Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.111058Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "eval_chat_model = HuggingFaceEndpoint(\n",
    "    model=\"prometheus-eval/prometheus-13b-v1.0\",\n",
    "    task='text-generation'\n",
    "    temperature=0, \n",
    ")\n",
    "evaluator_name = \"prometheus\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.113204Z",
     "iopub.status.idle": "2024-11-17T19:39:45.113689Z",
     "shell.execute_reply": "2024-11-17T19:39:45.113444Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.113423Z"
    }
   },
   "outputs": [],
   "source": [
    "RAW_KNOWLEDGE_BASE = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-17T19:39:45.115363Z",
     "iopub.status.idle": "2024-11-17T19:39:45.115825Z",
     "shell.execute_reply": "2024-11-17T19:39:45.115620Z",
     "shell.execute_reply.started": "2024-11-17T19:39:45.115593Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5994754,
     "sourceId": 9879418,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
