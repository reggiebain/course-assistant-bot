{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Evaluation\n",
    "- Create LLM generated set of Q&A about a document.\n",
    "- Use LLM critique/quality control these Q&A pairs.\n",
    "- Run another LLM on the pipeline with these Q&A pairs and study output to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.078066Z",
     "iopub.status.idle": "2024-11-28T05:47:07.078828Z",
     "shell.execute_reply": "2024-11-28T05:47:07.078357Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.078331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers sentence-transformers tqdm openpyxl pandas datasets ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.080675Z",
     "iopub.status.idle": "2024-11-28T05:47:07.081205Z",
     "shell.execute_reply": "2024-11-28T05:47:07.080948Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.080923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-huggingface langchain-community langchain-huggingface langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.082797Z",
     "iopub.status.idle": "2024-11-28T05:47:07.083346Z",
     "shell.execute_reply": "2024-11-28T05:47:07.083082Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.083056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.084904Z",
     "iopub.status.idle": "2024-11-28T05:47:07.085434Z",
     "shell.execute_reply": "2024-11-28T05:47:07.085204Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.085178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.086850Z",
     "iopub.status.idle": "2024-11-28T05:47:07.087384Z",
     "shell.execute_reply": "2024-11-28T05:47:07.087149Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.087123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_RAG_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.088980Z",
     "iopub.status.idle": "2024-11-28T05:47:07.089510Z",
     "shell.execute_reply": "2024-11-28T05:47:07.089277Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.089251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "#notebook_login()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data for Evaluating RAG\n",
    "- Import and preprocess a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.090713Z",
     "iopub.status.idle": "2024-11-28T05:47:07.091253Z",
     "shell.execute_reply": "2024-11-28T05:47:07.090997Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.090963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n",
    "#from langchain.docstore.document import Document\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Function to write outputs to file\n",
    "def write_output_to_file(output, filename):\n",
    "    # Ensure the output directory exists\n",
    "    out_dir = \"/kaggle/working/\"\n",
    "    #os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Define the full file path\n",
    "    file_path = os.path.join(out_dir, filename)\n",
    "    \n",
    "    # Write the output to the file\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(str(output))\n",
    "\n",
    "# Function to clean text (to remove unwanted line breaks within sentences)\n",
    "def clean_text(text):\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "# Function to load documents based on file type\n",
    "def load_documents(file_path):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        print(\"Loading PDF document...\")\n",
    "    elif file_extension.lower() == '.md':\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        print(\"Loading Markdown document...\")\n",
    "    elif file_extension.lower() == '.html':\n",
    "        loader = UnstructuredHTMLLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.092527Z",
     "iopub.status.idle": "2024-11-28T05:47:07.093219Z",
     "shell.execute_reply": "2024-11-28T05:47:07.092828Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.092802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the document and questions\n",
    "file_path = \"/kaggle/input/course-bot-data/documents/rbain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\n",
    "documents = load_documents(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.094871Z",
     "iopub.status.idle": "2024-11-28T05:47:07.095453Z",
     "shell.execute_reply": "2024-11-28T05:47:07.095202Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.095176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set some params\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add to list for later use\n",
    "docs_processed = []\n",
    "for doc in documents:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "# Write the chunks to file for manual study\n",
    "docs = []\n",
    "for i, doc in enumerate(split_docs):\n",
    "    output_str = f\"Chunk {i + 1}:\\n{doc.page_content}\\n\"\n",
    "    docs.append(output_str)\n",
    "\n",
    "write_output_to_file('\\n'.join(docs), 'chunks.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset of Q&A Pairs\n",
    "- Build synthetic dataset for testing the RAG pipeline\n",
    "- Mistral good for this see https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "- High performing model that has been used for synthetic question generation with 48.6 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.097353Z",
     "iopub.status.idle": "2024-11-28T05:47:07.097879Z",
     "shell.execute_reply": "2024-11-28T05:47:07.097643Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.097617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define LLM to generate synthetic Q&A pairs\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "qa_creation_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(model=qa_creation_model, timeout=120,)\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 1000},\"task\": \"text-generation\",},\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I tried to download the model locally below, but it exceeded maximum allowed disk space on Kaggle. On my local disk, I don't have access to a GPU, which is the best way to make use of it. HuggingFace obviously rate limits these large high performing models, so this part was quite tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.107455Z",
     "iopub.status.idle": "2024-11-28T05:47:07.109327Z",
     "shell.execute_reply": "2024-11-28T05:47:07.108953Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.108911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Alternate method, download LLM locally\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#qa_creation_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(qa_creation_model)\n",
    "#model = AutoModelForCausalLM.from_pretrained(qa_creation_model)\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "#def call_local_llm(prompt: str):\n",
    "#    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#    outputs = model.generate(inputs.input_ids, max_new_tokens=1000)\n",
    "#    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we write a \"factoid\" question prompt (see https://www-cs-faculty.stanford.edu/people/mengqiu/publication/LSII-LitReview.pdf) to generate synthetic data. Such questions ask for a specific fact type answer that is concise. Straightforward task for LLM and reasonable for syllabus over reasoning/advanced analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.111083Z",
     "iopub.status.idle": "2024-11-28T05:47:07.111862Z",
     "shell.execute_reply": "2024-11-28T05:47:07.111543Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.111455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qa_creation_prompt = \"\"\"\n",
    "### Instructions\n",
    "Your task is to generate a factoid question and its corresponding answer based on the provided context below.\n",
    "The factoid question should\n",
    "1. Be answerable with a specific, concise piece of factual information from the context.\n",
    "2. Be written in a natural, user-friendly style, similar to what users might input in a search engine.\n",
    "3. Avoid mentioning terms like \"context,\" \"passage,\" or \"according to the text.\"\n",
    "\n",
    "The answer to the factoid questions should\n",
    "1. Be short, precise, and derived directly from the context.\n",
    "2. Avoid adding any information that is not explicitly present in the context.\n",
    "\n",
    "### Formatting\n",
    "Provide your response exactly as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (insert your factoid question here)\n",
    "Answer: (insert your answer to the factoid question here)\n",
    "\n",
    "### Provided Context\n",
    "Below is the context upon which to base the factoid question and its corresponding answer\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.113747Z",
     "iopub.status.idle": "2024-11-28T05:47:07.114365Z",
     "shell.execute_reply": "2024-11-28T05:47:07.114070Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.114039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "#N_GENERATIONS = 30\n",
    "N_GENERATIONS = min(30, len(docs_processed))\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    time.sleep(2)\n",
    "    output_QA_couple = call_llm(llm_client, qa_creation_prompt.format(context=sampled_context.page_content))\n",
    "    #output_QA_couple = call_local_llm(qa_creation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.116139Z",
     "iopub.status.idle": "2024-11-28T05:47:07.116725Z",
     "shell.execute_reply": "2024-11-28T05:47:07.116450Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.116422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save outputs to readable csv file so we can read manually\n",
    "outputs_df = pd.DataFrame(outputs)\n",
    "outputs_df.to_csv('generated_qa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.118176Z",
     "iopub.status.idle": "2024-11-28T05:47:07.118825Z",
     "shell.execute_reply": "2024-11-28T05:47:07.118543Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.118513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.120548Z",
     "iopub.status.idle": "2024-11-28T05:47:07.121149Z",
     "shell.execute_reply": "2024-11-28T05:47:07.120867Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.120837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test one of the questions\n",
    "print(f'Question: {outputs_df.iloc[0,:].question}')\n",
    "print(f'Answer: {outputs_df.iloc[0,:].answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Critique Models\n",
    "- Establish some evaluation metrics for Q&A set, create prompts\n",
    "- Use LLM to evaluate the Q&A and create scores\n",
    "- Filter our Q&A set quality based on those scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Critiquing Questions\n",
    "Ref. https://docs.ragas.io/en/latest/concepts/metrics/index.html \n",
    "- Groundedness: can the question be answered from the given context?\n",
    "- Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n",
    "- Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n",
    "- Faithfulness: number of claims in the generated answer that can be inferred from given context / total number of claims in generated answer\n",
    "-   Set of claims from generated answer identified\n",
    "-   Each claim cross checked within the context.\n",
    "- https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.122993Z",
     "iopub.status.idle": "2024-11-28T05:47:07.123693Z",
     "shell.execute_reply": "2024-11-28T05:47:07.123409Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.123378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "faithfulness_prompt = \"\"\"\n",
    "Your task is to evaluate the **faithfulness** of a question based on the provided context.\n",
    "\n",
    "### Faithfulness Definition:\n",
    "Faithfulness is defined as the proportion of claims in the generated answer that can be directly inferred from the context. Specifically:\n",
    "- **Faithful Claims**: Claims in the answer that are explicitly stated or logically inferable from the given context.\n",
    "- **Unfaithful Claims**: Claims in the answer that are not supported by the context, including hallucinated or extraneous information.\n",
    "\n",
    "The faithfulness score is calculated as:\n",
    "`Faithfulness Score = (Number of Faithful Claims) / (Total Number of Claims in the Answer)`\n",
    "\n",
    "### Instructions:\n",
    "1. Carefully review the provided context, question, and generated answer.\n",
    "2. Identify all individual claims made in the generated answer.\n",
    "3. For each claim, determine if it is **faithful** (supported by the context) or **unfaithful** (unsupported or hallucinated).\n",
    "4. Compute the faithfulness score and provide a brief explanation for the score.\n",
    "\n",
    "### Format:\n",
    "Provide your response in the following format:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "### Provided Question and Context\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \n",
    "\n",
    "\"\"\"\n",
    "groundedness_prompt = \"\"\"\n",
    "### Instructions\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "relevance_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "standalone_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.125371Z",
     "iopub.status.idle": "2024-11-28T05:47:07.125940Z",
     "shell.execute_reply": "2024-11-28T05:47:07.125679Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.125649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "# Loop over outputs df and grab the scores\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(llm_client,groundedness_prompt.format(context=output[\"context\"], question=output[\"question\"])),\n",
    "        \"relevance\": call_llm(llm_client,relevance_prompt.format(question=output[\"question\"])),\n",
    "        \"standalone\": call_llm(llm_client,standalone_prompt.format(question=output[\"question\"])),\n",
    "        #\"faithfulness\": call_llm(llm_client, faithfulness_prompt.format(context=output['context'], question=output['question'])),\n",
    "    }\n",
    "    try:\n",
    "        # Loop over each critique criterian and evaluation rating splitting on output string\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update({f\"{criterion}_score\": score,\n",
    "                           f\"{criterion}_eval\": eval,})\n",
    "    except Exception as e:\n",
    "        print(f\"{criterion=}\")\n",
    "        print(f\"{evaluation=}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.127427Z",
     "iopub.status.idle": "2024-11-28T05:47:07.128075Z",
     "shell.execute_reply": "2024-11-28T05:47:07.127760Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.127730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Filter q&A pairs based on critique scores and log\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "# Save before filtering\n",
    "#generated_questions.to_csv('generated_qa_critique_no_filter.csv', index=False)\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]].iloc[0:5,:])\n",
    "\n",
    "# Filter to make sure relatively relevant responses\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 1)\n",
    "    & (generated_questions[\"relevance_score\"] >= 1)\n",
    "    & (generated_questions[\"standalone_score\"] >= 1)\n",
    "#    & (generated_questions['faithfulness_score'] >= 1)\n",
    "]\n",
    "\n",
    "invalid_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] < 1)\n",
    "    & (generated_questions[\"relevance_score\"] < 1)\n",
    "    & (generated_questions[\"standalone_score\"] < 1)\n",
    " #   & (generated_questions['faithfulness_score'] < 1)\n",
    "]\n",
    "\n",
    "# Save filtered and killed data\n",
    "generated_questions.to_csv('generated_qa_critique_filtered.csv', index=False)\n",
    "invalid_questions.to_csv('generated_qa_critique_invalid.csv', index=False)\n",
    "\n",
    "# Preview filtered dataset\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]].iloc[0:5,:])\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import/Build RAG Pipeline for Evaluation\n",
    "Create methods to:\n",
    "1. Split documents and preprocess\n",
    "2. Create vector store and fill with embeddings\n",
    "3. Build retriever with prompt to get the context and feed to LLM\n",
    "4. Run tests to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.129730Z",
     "iopub.status.idle": "2024-11-28T05:47:07.130306Z",
     "shell.execute_reply": "2024-11-28T05:47:07.130029Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.129994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Implement splitting of docs via text_splitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define a method to create or load the embeddings using a FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.131821Z",
     "iopub.status.idle": "2024-11-28T05:47:07.132388Z",
     "shell.execute_reply": "2024-11-28T05:47:07.132136Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.132106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Method to load embeddins and create vectore store\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "def load_embeddings(langchain_docs, chunk_size, embedding_model_name: Optional[str] = \"thenlper/gte-small\",) -> FAISS:\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        #model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE,\n",
    "            #allow_dangerous_deserialization=True\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.133775Z",
     "iopub.status.idle": "2024-11-28T05:47:07.134388Z",
     "shell.execute_reply": "2024-11-28T05:47:07.134138Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.134108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a \"reader\" model that will be used to test our RAG pipeline. This should have mostly standard params, but have a very low temperature, since we want it to objectively evaluate results from another LLM.\n",
    "Models: \n",
    "1. https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
    "2. https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "3. https://huggingface.co/colbert-ir/colbertv2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.136363Z",
     "iopub.status.idle": "2024-11-28T05:47:07.136919Z",
     "shell.execute_reply": "2024-11-28T05:47:07.136651Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.136622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Zephyr 7b from Mistral AI\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "\n",
    "# LLama 3.1 8B Instruct from Meta\n",
    "#repo_id = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "#READER_MODEL_NAME = 'Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Declare some parameters for the RAG Q&A LLM\n",
    "reader_model_params = {\"max_new_tokens\": 512, \"top_k\": 30,\"temperature\": 0.1,\"repetition_penalty\": 1.03,}\n",
    "\n",
    "reader_llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\", \n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    model_kwargs = reader_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.138289Z",
     "iopub.status.idle": "2024-11-28T05:47:07.138845Z",
     "shell.execute_reply": "2024-11-28T05:47:07.138576Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.138548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "def answer_with_rag(question: str, llm: LLM, knowledge_index: VectorStore, \n",
    "                    reranker: Optional[RAGPretrainedModel] = None, num_retrieved_docs: int = 30,\n",
    "                    num_docs_final: int = 7,) -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results using RAGatoulli ColBERT model\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "    #answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.140440Z",
     "iopub.status.idle": "2024-11-28T05:47:07.141124Z",
     "shell.execute_reply": "2024-11-28T05:47:07.140726Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.140698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "# Method to run rag on set of questions\n",
    "def run_rag_tests(eval_dataset, llm, knowledge_index, output_file,\n",
    "                  reranker: Optional[RAGPretrainedModel] = None, verbose: Optional[bool] = True,\n",
    "                  test_settings: Optional[str] = None,):\n",
    "    \n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    # loop over the q&a pairs in the eval dataset \n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        # Optionally include the details about the model settings throughout the pipeline\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        # Print everything to an output json file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.143205Z",
     "iopub.status.idle": "2024-11-28T05:47:07.143800Z",
     "shell.execute_reply": "2024-11-28T05:47:07.143517Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.143482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (ChatPromptTemplate,HumanMessagePromptTemplate,)\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"), \n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Judge Agent\n",
    "Next we declare a judge agent to evaluate the results and a function to evaluate the a&a pairs for various metrics like faithfulness. We'll look at:\n",
    "1. OpenAI GPT 4 1106 (Proprietary) - https://community.openai.com/t/gpt-4-1106-preview-vs-gpt-4/588424\n",
    "2. FlowAI Judge - https://huggingface.co/flowaicom/Flow-Judge-v0.1-AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.148346Z",
     "iopub.status.idle": "2024-11-28T05:47:07.148980Z",
     "shell.execute_reply": "2024-11-28T05:47:07.148688Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.148655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Choose a model for judge agent\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEndpoint\n",
    "\n",
    "# Uncomment to use best model available GPT4\n",
    "eval_chat_model = ChatOpenAI(model='N', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = 'gpt4'\n",
    "\n",
    "# Uncomment to use open source much smaller FlowAI judge\n",
    "\n",
    "#model_kwargs = {'temperature': 0.1, 'repetition_penalty':1.03}\n",
    "#eval_chat_model = ChatHuggingFace(\n",
    "#    llm= HuggingFacePipeline.from_model_id(\n",
    "#        #endpoint_url = \"https://api-inference.huggingface.co/models/flowaicom/Flow-Judge-v0.1\",\n",
    "#        model_id='flowaicom/Flow-Judge-v0.1',\n",
    "#        task='text-generation',\n",
    "#        model_kwargs=model_kwargs,\n",
    "#        #temperature=0.1,\n",
    "#        #reptition_penalty=1.03,\n",
    "#    )\n",
    "#)    \n",
    "#evaluator_name = \"flow-judge\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.150272Z",
     "iopub.status.idle": "2024-11-28T05:47:07.150828Z",
     "shell.execute_reply": "2024-11-28T05:47:07.150571Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.150543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make function to evaluate qa pairs\n",
    "def evaluate_answers(answer_path, eval_chat_model, evaluator_name, \n",
    "                     evaluation_prompt_template: ChatPromptTemplate,) -> None:\n",
    "    \n",
    "    answers = []\n",
    "    # Only work on files where answer_path exists (i.e. where the q&a pairs are)\n",
    "    if os.path.isfile(answer_path):\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        # Move on if score is already recorded\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Methods to Test + Finetune\n",
    "- Loop over hyperparameters and run.\n",
    "1. Chunk size\n",
    "2. Embedding models and/or critique/evaluation models\n",
    "3. Rerank of retrieved contexts or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.152496Z",
     "iopub.status.idle": "2024-11-28T05:47:07.153108Z",
     "shell.execute_reply": "2024-11-28T05:47:07.152828Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.152797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RAW_KNOWLEDGE_BASE = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.154531Z",
     "iopub.status.idle": "2024-11-28T05:47:07.155138Z",
     "shell.execute_reply": "2024-11-28T05:47:07.154833Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.154805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=reader_llm,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.157275Z",
     "iopub.status.idle": "2024-11-28T05:47:07.157855Z",
     "shell.execute_reply": "2024-11-28T05:47:07.157596Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.157566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use glob to recursively go through all of the json output files we dumped\n",
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "    \n",
    "# Make dataframe of output results\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.160023Z",
     "iopub.status.idle": "2024-11-28T05:47:07.160685Z",
     "shell.execute_reply": "2024-11-28T05:47:07.160386Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.160355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.162998Z",
     "iopub.status.idle": "2024-11-28T05:47:07.163661Z",
     "shell.execute_reply": "2024-11-28T05:47:07.163364Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.163331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result[\"eval_score_gpt4\"] = result[\"eval_score_gpt4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_gpt4\"] = (result[\"eval_score_gpt4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.165698Z",
     "iopub.status.idle": "2024-11-28T05:47:07.166335Z",
     "shell.execute_reply": "2024-11-28T05:47:07.166029Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.165996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_gpt4\"].mean()\n",
    "average_scores.sort_values()\n",
    "average_scores.to_csv('average_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.168489Z",
     "iopub.status.idle": "2024-11-28T05:47:07.169207Z",
     "shell.execute_reply": "2024-11-28T05:47:07.168922Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.168890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(result.eval_score_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-28T05:47:07.170931Z",
     "iopub.status.idle": "2024-11-28T05:47:07.171512Z",
     "shell.execute_reply": "2024-11-28T05:47:07.171258Z",
     "shell.execute_reply.started": "2024-11-28T05:47:07.171229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r 'rag_eval_results.zip' '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5994754,
     "sourceId": 9879418,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
