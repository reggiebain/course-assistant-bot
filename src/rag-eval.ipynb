{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9879418,"sourceType":"datasetVersion","datasetId":5994754}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl pandas datasets langchain-community ragatouille","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:35:48.498893Z","iopub.execute_input":"2024-11-17T19:35:48.499423Z","iopub.status.idle":"2024-11-17T19:36:05.164789Z","shell.execute_reply.started":"2024-11-17T19:35:48.499371Z","shell.execute_reply":"2024-11-17T19:36:05.163138Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:36:05.167643Z","iopub.execute_input":"2024-11-17T19:36:05.168073Z","iopub.status.idle":"2024-11-17T19:36:05.251619Z","shell.execute_reply.started":"2024-11-17T19:36:05.168030Z","shell.execute_reply":"2024-11-17T19:36:05.250428Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Optional, List, Tuple\nimport json\nimport datasets\nimport os\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:36:05.253133Z","iopub.execute_input":"2024-11-17T19:36:05.253517Z","iopub.status.idle":"2024-11-17T19:36:05.324072Z","shell.execute_reply.started":"2024-11-17T19:36:05.253437Z","shell.execute_reply":"2024-11-17T19:36:05.322801Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#from huggingface_hub import notebook_login\nfrom huggingface_hub import login\n#HF_TOKEN = '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'\nHF_TOKEN = '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'\n#notebook_login()\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:44:53.516558Z","iopub.execute_input":"2024-11-17T19:44:53.516980Z","iopub.status.idle":"2024-11-17T19:44:53.674349Z","shell.execute_reply.started":"2024-11-17T19:44:53.516941Z","shell.execute_reply":"2024-11-17T19:44:53.673179Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n#from langchain.docstore.document import Document\nfrom langchain.schema import Document\n\n# Function to clean text (to remove unwanted line breaks within sentences)\ndef clean_text(text):\n    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n\n# Function to load documents based on file type\ndef load_documents(file_path):\n    _, file_extension = os.path.splitext(file_path)\n\n    if file_extension.lower() == '.pdf':\n        loader = PyPDFLoader(file_path)\n        print(\"Loading PDF document...\")\n    elif file_extension.lower() == '.md':\n        loader = UnstructuredMarkdownLoader(file_path)\n        print(\"Loading Markdown document...\")\n    elif file_extension.lower() == '.html':\n        loader = UnstructuredHTMLLoader(file_path)\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n    \n    documents = loader.load()\n    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:44:56.567592Z","iopub.execute_input":"2024-11-17T19:44:56.568053Z","iopub.status.idle":"2024-11-17T19:44:56.644255Z","shell.execute_reply.started":"2024-11-17T19:44:56.568006Z","shell.execute_reply":"2024-11-17T19:44:56.642999Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# Load the document and questions\nfile_path = \"/kaggle/input/course-bot-data/bain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\ndocuments = load_documents(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:57:12.494091Z","iopub.execute_input":"2024-11-17T19:57:12.494588Z","iopub.status.idle":"2024-11-17T19:57:13.227756Z","shell.execute_reply.started":"2024-11-17T19:57:12.494539Z","shell.execute_reply":"2024-11-17T19:57:13.226605Z"}},"outputs":[{"name":"stdout","text":"Loading PDF document...\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Set some params\nCHUNK_SIZE = 2000\nCHUNK_OVERLAP = 200\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\nsplit_docs = text_splitter.split_documents(documents)\n\ndocs_processed = []\nfor doc in documents:\n    docs_processed += text_splitter.split_documents([doc])\n\nprint(f\"Total chunks created: {len(split_docs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:57:38.984691Z","iopub.execute_input":"2024-11-17T19:57:38.985184Z","iopub.status.idle":"2024-11-17T19:57:39.088178Z","shell.execute_reply.started":"2024-11-17T19:57:38.985107Z","shell.execute_reply":"2024-11-17T19:57:39.086799Z"}},"outputs":[{"name":"stdout","text":"Total chunks created: 13\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"docs_processed[0].page_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:03:01.986738Z","iopub.execute_input":"2024-11-17T20:03:01.987203Z","iopub.status.idle":"2024-11-17T20:03:02.059305Z","shell.execute_reply.started":"2024-11-17T20:03:01.987157Z","shell.execute_reply":"2024-11-17T20:03:02.058089Z"}},"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"'I.\\nCourse\\nDescription\\n1.\\nCourse\\nSummary\\na.\\nPHY\\n161/PHYS\\n215\\nGeneral\\nPhysics\\nI\\nis\\nan\\nalgebra-based\\nintroduction\\nto\\nmechanics,\\nthermodynamics,\\nand\\nwaves.\\nTopics\\ninclude\\nmotion\\nin\\none\\nand\\ntwo\\ndimensions,\\nNewton’s\\nlaws\\nof\\nmotion,\\nequilibrium,\\nwork,\\nenergy,\\nmomentum,\\nrotational\\nmotion,\\ngravity,\\nheat,\\nwaves,\\nand\\nsound.\\nExamples\\nfrom\\nmedicine\\nand\\nbiology\\nwill\\nbe\\nincluded\\nwhenever\\npossible.\\n2.\\nCollege\\nCredit\\nHours\\n(Dual-Enrollment)\\na.\\nThis\\ncourse\\nis\\ndual\\nenrolled\\nwith\\nPHYS\\n215\\nGeneral\\nPhysics\\nI\\nat\\nFrancis\\nMarion\\nUniversity\\n(FMU)\\nand\\ntaught\\nby\\na\\nGSSM\\ninstructor.\\nStudents\\nwill\\neach\\nhave\\na\\nFMU\\ntranscript\\nwith\\ntheir\\noverall\\ngrade\\nearned\\nin\\nthis\\ncourse.\\nStudents\\nmay\\nearn\\nup\\nto\\n4\\ncollege\\ncredit\\nhours\\ndepending\\non\\ntheir\\ngrade\\nand\\nthe\\ntransfer\\npolicies\\nof\\ntheir\\ncollege/university.\\nRefer\\nto\\nthe\\nDual \\nEnrollment \\nFAQ \\nin \\nthe \\nCourse \\nCatalog \\nfor\\nmore\\ninformation.\\n3.\\nLearning\\nOutcomes\\na.\\nUpon\\ncompletion\\nof\\nthis\\ncourse,\\nstudents\\nwill\\nbe\\nable\\nto:\\nb.\\nApply\\nthe\\nlaws\\nof\\nclassical\\nNewtonian\\nmechanics\\n(motion,\\nforce,\\nenergy,\\nmomentum,\\nand\\ngravitation)\\nto\\nsolve\\nproblems\\ninvolving\\nstatic\\nsystems,\\nmotion\\nwith\\nconstant\\nacceleration,\\nand\\nrotational\\nmotion.\\nc.\\nUse\\ntechniques\\nof\\ngraphical\\nanalysis\\nto\\nclassify\\nand\\nmodel\\nphysical\\nphenomena\\nand\\napply\\nphysical\\nmeaning\\nto\\nmathematical\\ntechniques\\nfrom\\nalgebra,\\npre-calculus,\\nand\\ntrigonometry.\\nPage\\n1\\nof\\n8\\nCourse\\nPHY\\n161\\n-\\nGeneral\\nPhysics\\n1\\n(Dual\\nEnrolled\\nw/\\nFMU)\\nSemester\\nFall\\n2024\\nLecture\\nTuThFr\\n1-2\\n|\\nTuThFr\\n2-3\\n|\\nTuThFr\\n3-4\\nLab\\nMo\\n2-3\\n|\\nTu\\n10-12\\n|\\nWe\\n2-4\\nLocation\\nC-107\\nand\\nhttps://gssm.zoom.us/j/8622923374\\nInstructor\\nDr.\\nReginald\\nBain\\n| ✉\\nrbain@governors.school\\n📞\\n843-383-3900\\n| 🏢\\nOﬃce:\\nB166/Zoom\\n(862-292-3374)\\n🕐\\nOﬃce\\nHours:\\nBy\\nappointment\\nOR\\nMoWe\\n10-11,\\n1-2,\\nThFr\\n10-11'"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"from huggingface_hub import InferenceClient\n\nrepo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\nllm_client = InferenceClient(\n    model=repo_id,\n    timeout=120,\n)\n\ndef call_llm(inference_client: InferenceClient, prompt: str):\n    response = inference_client.post(\n        json={\n            \"inputs\": prompt,\n            \"parameters\": {\"max_new_tokens\": 1000},\n            \"task\": \"text-generation\",\n        },\n    )\n    return json.loads(response.decode())[0][\"generated_text\"]\n\n\ncall_llm(llm_client, \"This is a test context\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:57:47.199709Z","iopub.execute_input":"2024-11-17T19:57:47.200160Z","iopub.status.idle":"2024-11-17T19:57:47.355803Z","shell.execute_reply.started":"2024-11-17T19:57:47.200103Z","shell.execute_reply":"2024-11-17T19:57:47.354632Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"QA_generation_prompt = \"\"\"\nYour task is to write a factoid question and an answer given a context.\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\nYour factoid question should be formulated in the same style as questions users could ask in a search engine.\nThis means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n\nProvide your answer as follows:\n\nOutput:::\nFactoid question: (your factoid question)\nAnswer: (your answer to the factoid question)\n\nNow here is the context.\n\nContext: {context}\\n\nOutput:::\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:57:49.938461Z","iopub.execute_input":"2024-11-17T19:57:49.938935Z","iopub.status.idle":"2024-11-17T19:57:50.009972Z","shell.execute_reply.started":"2024-11-17T19:57:49.938886Z","shell.execute_reply":"2024-11-17T19:57:50.008648Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"import random\nimport time\nN_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n\nprint(f\"Generating {N_GENERATIONS} QA couples...\")\n\noutputs = []\nfor sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n    # Generate QA couple\n    time.sleep(1)\n    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n    try:\n        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n        answer = output_QA_couple.split(\"Answer: \")[-1]\n        assert len(answer) < 300, \"Answer is too long\"\n        outputs.append(\n            {\n                \"context\": sampled_context.page_content,\n                \"question\": question,\n                \"answer\": answer,\n                \"source_doc\": sampled_context.metadata[\"source\"],\n            }\n        )\n    except:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:57:51.981095Z","iopub.execute_input":"2024-11-17T19:57:51.981664Z","iopub.status.idle":"2024-11-17T20:00:23.038743Z","shell.execute_reply.started":"2024-11-17T19:57:51.981614Z","shell.execute_reply":"2024-11-17T20:00:23.037215Z"}},"outputs":[{"name":"stdout","text":"Generating 10 QA couples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6c08b9ee884e6196ea407237a1f82c"}},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"outputs_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:00:23.077474Z","iopub.execute_input":"2024-11-17T20:00:23.077829Z","iopub.status.idle":"2024-11-17T20:00:23.155602Z","shell.execute_reply.started":"2024-11-17T20:00:23.077790Z","shell.execute_reply":"2024-11-17T20:00:23.154227Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n0                           a.\\nDone\\nvia\\nWebAssign,\\nroughly\\n1-2\\nassignments\\nwill\\nbe\\ndue\\nper\\nweek\\nconstituting\\n~10-20\\nproblems/week.\\nDeadlines\\nare\\nalways\\nvisible\\nonline.\\nStudents\\nare\\nresponsible\\nfor\\nchecking\\nWebAssign/Canvas\\nfor\\ndue\\ndates.\\nb.\\nStudents\\nSHOULD\\nNOT\\nleave\\nthese\\nassignments\\nuntil\\nthe\\nlast\\nminute\\nand\\nshould\\nindividually\\nbudget\\nseveral\\nhours\\nof\\ntime\\nto\\nwork\\non\\nproblems\\neach\\nweek\\nthat\\nworks\\nwith\\ntheir\\nschedule.\\nTypical\\nHW\\ndeadlines\\nwill\\nbe\\nTuesday/Friday\\nevenings.\\nc.\\nA\\nlist\\nof\\ndaily\\nreading\\nassignments\\nis\\nposted\\non\\nCanvas.\\nStudents\\nshould\\nread\\nthe\\nassigned\\nsections\\nbefore\\neach\\nclass\\nperiod.\\n4.\\nMissed/Late\\nAssignments\\na.\\nMissed/late \\nassignments \\nwithout \\nan \\no ﬃ cial \\nexcused \\nabsence \\nwill \\nreceive \\nno \\ncredit. \\nIf\\na\\nstudent\\nwill\\nmiss\\nclass\\nbecause\\nof\\nan\\nexcused\\nabsence,\\nstudents\\nmay\\nrequest\\nan\\nextension\\nfrom\\nthe\\ninstructor\\nbeforehand.\\nThey\\ncan\\nrequest\\nthis\\nextension\\ndirectly\\non\\nWebAssign.\\nExtensions \\nwill \\nonly \\nbe \\nprovided \\nfor \\nexcused \\nabsences. \\nOtherwise, \\nno \\nlate \\nwork \\nis \\naccepted. \\nb.\\nValid   \n1  and\\nindividual\\nwork.\\nCopied\\nwork\\nwill\\nnot\\nbe\\naccepted\\nand\\nwill\\nbe\\nassigned\\na\\n0%.\\nDuring\\nquizzes/exams,\\nif\\na\\nphone\\nis\\nseen\\nbeing\\nused\\nor\\ncheating\\nis\\nobserved,\\na\\n0%\\nwill\\nbe\\nassigned.\\n8.\\nInclusion\\na.\\nIn\\nthis\\nclass,\\nwe\\nwill\\nall\\nmaintain\\na\\ncommunity\\nin\\nwhich\\nevery\\nperson\\nfeels\\nwelcomed,\\nvalued,\\nand\\nrespected.\\nIt\\nis\\nexpected\\nthat\\nyou\\nwill\\ntreat\\nyour\\nclassmates\\ngraciously\\nand\\nresponsibly\\nand\\nthat\\nyou\\nwill\\nshow\\nrespect\\nfor\\ndiverse\\nperspectives,\\nespecially\\nin\\nterms\\nof\\nrace,\\ngender,\\nsexuality,\\nreligion,\\nnational\\norigin, disability\\nstatus, and\\nsocioeconomic\\nstatus.\\nVI.\\nCourse\\nSchedule\\n1.\\nExams\\n&\\nImportant\\nDates\\na.\\nAlthough\\nTEST\\nDATES\\nARE\\nSUBJECT\\nTO\\nCHANGE\\n,\\nI\\nestimate\\nthat\\nthe\\n4\\nmajor\\ntests\\nwill\\noccur\\nroughly\\nthe\\nweek\\nof\\neach\\nof\\nthe\\ndates\\nbelow.\\nOne-week\\nnotice\\nwill\\nalways\\nbe\\ngiven\\nwhen\\nexam\\ndates\\nare\\nﬁnalized.\\nThese\\ndates\\nwill\\ndepend\\non\\nhow\\nthe\\nclass\\nis\\nperforming,\\ndays\\nmissed\\ndue\\nto\\nextreme\\nweather,\\netc.\\n1.\\nExam\\n1\\n(App.\\nA,\\nCh\\n0-2)\\n—\\nTuesday,\\nSeptember\\n3rd\\n2.\\nExam\\n2\\n(Ch\\n3-4)\\n—\\nTuesday,\\nOctober   \n\n                                                                    question  \\\n0  What is the name of the dragon that is preventing him from completing his   \n1                                                  (your factoid question)\\n   \n\n                                                                                                                                                                         answer  \\\n0  The name of the dragon that is preventing him from completing his assignment.\\n\\nFactoid question: What is the name of the dragon that is preventing him from completing his   \n1                                                                                                                                                                      Pourquoi   \n\n                                        source_doc  \n0  /kaggle/input/course-bot-data/bain_syllabus.pdf  \n1  /kaggle/input/course-bot-data/bain_syllabus.pdf  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>source_doc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a.\\nDone\\nvia\\nWebAssign,\\nroughly\\n1-2\\nassignments\\nwill\\nbe\\ndue\\nper\\nweek\\nconstituting\\n~10-20\\nproblems/week.\\nDeadlines\\nare\\nalways\\nvisible\\nonline.\\nStudents\\nare\\nresponsible\\nfor\\nchecking\\nWebAssign/Canvas\\nfor\\ndue\\ndates.\\nb.\\nStudents\\nSHOULD\\nNOT\\nleave\\nthese\\nassignments\\nuntil\\nthe\\nlast\\nminute\\nand\\nshould\\nindividually\\nbudget\\nseveral\\nhours\\nof\\ntime\\nto\\nwork\\non\\nproblems\\neach\\nweek\\nthat\\nworks\\nwith\\ntheir\\nschedule.\\nTypical\\nHW\\ndeadlines\\nwill\\nbe\\nTuesday/Friday\\nevenings.\\nc.\\nA\\nlist\\nof\\ndaily\\nreading\\nassignments\\nis\\nposted\\non\\nCanvas.\\nStudents\\nshould\\nread\\nthe\\nassigned\\nsections\\nbefore\\neach\\nclass\\nperiod.\\n4.\\nMissed/Late\\nAssignments\\na.\\nMissed/late \\nassignments \\nwithout \\nan \\no ﬃ cial \\nexcused \\nabsence \\nwill \\nreceive \\nno \\ncredit. \\nIf\\na\\nstudent\\nwill\\nmiss\\nclass\\nbecause\\nof\\nan\\nexcused\\nabsence,\\nstudents\\nmay\\nrequest\\nan\\nextension\\nfrom\\nthe\\ninstructor\\nbeforehand.\\nThey\\ncan\\nrequest\\nthis\\nextension\\ndirectly\\non\\nWebAssign.\\nExtensions \\nwill \\nonly \\nbe \\nprovided \\nfor \\nexcused \\nabsences. \\nOtherwise, \\nno \\nlate \\nwork \\nis \\naccepted. \\nb.\\nValid</td>\n      <td>What is the name of the dragon that is preventing him from completing his</td>\n      <td>The name of the dragon that is preventing him from completing his assignment.\\n\\nFactoid question: What is the name of the dragon that is preventing him from completing his</td>\n      <td>/kaggle/input/course-bot-data/bain_syllabus.pdf</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>and\\nindividual\\nwork.\\nCopied\\nwork\\nwill\\nnot\\nbe\\naccepted\\nand\\nwill\\nbe\\nassigned\\na\\n0%.\\nDuring\\nquizzes/exams,\\nif\\na\\nphone\\nis\\nseen\\nbeing\\nused\\nor\\ncheating\\nis\\nobserved,\\na\\n0%\\nwill\\nbe\\nassigned.\\n8.\\nInclusion\\na.\\nIn\\nthis\\nclass,\\nwe\\nwill\\nall\\nmaintain\\na\\ncommunity\\nin\\nwhich\\nevery\\nperson\\nfeels\\nwelcomed,\\nvalued,\\nand\\nrespected.\\nIt\\nis\\nexpected\\nthat\\nyou\\nwill\\ntreat\\nyour\\nclassmates\\ngraciously\\nand\\nresponsibly\\nand\\nthat\\nyou\\nwill\\nshow\\nrespect\\nfor\\ndiverse\\nperspectives,\\nespecially\\nin\\nterms\\nof\\nrace,\\ngender,\\nsexuality,\\nreligion,\\nnational\\norigin, disability\\nstatus, and\\nsocioeconomic\\nstatus.\\nVI.\\nCourse\\nSchedule\\n1.\\nExams\\n&amp;\\nImportant\\nDates\\na.\\nAlthough\\nTEST\\nDATES\\nARE\\nSUBJECT\\nTO\\nCHANGE\\n,\\nI\\nestimate\\nthat\\nthe\\n4\\nmajor\\ntests\\nwill\\noccur\\nroughly\\nthe\\nweek\\nof\\neach\\nof\\nthe\\ndates\\nbelow.\\nOne-week\\nnotice\\nwill\\nalways\\nbe\\ngiven\\nwhen\\nexam\\ndates\\nare\\nﬁnalized.\\nThese\\ndates\\nwill\\ndepend\\non\\nhow\\nthe\\nclass\\nis\\nperforming,\\ndays\\nmissed\\ndue\\nto\\nextreme\\nweather,\\netc.\\n1.\\nExam\\n1\\n(App.\\nA,\\nCh\\n0-2)\\n—\\nTuesday,\\nSeptember\\n3rd\\n2.\\nExam\\n2\\n(Ch\\n3-4)\\n—\\nTuesday,\\nOctober</td>\n      <td>(your factoid question)\\n</td>\n      <td>Pourquoi</td>\n      <td>/kaggle/input/course-bot-data/bain_syllabus.pdf</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"outputs_df = pd.DataFrame(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:12.219849Z","iopub.execute_input":"2024-11-17T19:46:12.221205Z","iopub.status.idle":"2024-11-17T19:46:12.293339Z","shell.execute_reply.started":"2024-11-17T19:46:12.221142Z","shell.execute_reply":"2024-11-17T19:46:12.291903Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"print(f\"Question: {outputs_df.iloc[0,:]['question']}\")\nprint(f\"Answer: {outputs_df.iloc[0,:]['answer']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:12.295248Z","iopub.execute_input":"2024-11-17T19:46:12.295786Z","iopub.status.idle":"2024-11-17T19:46:12.368448Z","shell.execute_reply.started":"2024-11-17T19:46:12.295730Z","shell.execute_reply":"2024-11-17T19:46:12.366920Z"}},"outputs":[{"name":"stdout","text":"Question: What is the name of the dragon that is preventing him from completing his\nAnswer: The name of the dragon that is preventing him from completing his assignment.\n\nFactoid question: What is the name of the dragon that is preventing him from completing his\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"outputs_df[['question', 'answer']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:12.369958Z","iopub.execute_input":"2024-11-17T19:46:12.370375Z","iopub.status.idle":"2024-11-17T19:46:12.447903Z","shell.execute_reply.started":"2024-11-17T19:46:12.370332Z","shell.execute_reply":"2024-11-17T19:46:12.446476Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                                                                    question  \\\n0  What is the name of the dragon that is preventing him from completing his   \n1                                                  (your factoid question)\\n   \n\n                                                                                                                                                                         answer  \n0  The name of the dragon that is preventing him from completing his assignment.\\n\\nFactoid question: What is the name of the dragon that is preventing him from completing his  \n1                                                                                                                                                                      Pourquoi  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the name of the dragon that is preventing him from completing his</td>\n      <td>The name of the dragon that is preventing him from completing his assignment.\\n\\nFactoid question: What is the name of the dragon that is preventing him from completing his</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(your factoid question)\\n</td>\n      <td>Pourquoi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":69},{"cell_type":"markdown","source":"- Groundedness: can the question be answered from the given context?\n- Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n- Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n- Faithfulness: number of claims in the generated answer that can be inferred from given context / total number of claims in generated answer\n-   Set of claims from generated answer identified\n-   Each claim cross checked within the context.\n- https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/","metadata":{}},{"cell_type":"code","source":"question_groundedness_critique_prompt = \"\"\"\nYou will be given a context and a question.\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here are the question and context.\n\nQuestion: {question}\\n\nContext: {context}\\n\nAnswer::: \"\"\"\n\nquestion_relevance_critique_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"\n\nquestion_standalone_critique_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how context-independant this question is.\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\nFor instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:20.871157Z","iopub.execute_input":"2024-11-17T19:46:20.871574Z","iopub.status.idle":"2024-11-17T19:46:20.943897Z","shell.execute_reply.started":"2024-11-17T19:46:20.871536Z","shell.execute_reply":"2024-11-17T19:46:20.942723Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"print(\"Generating critique for each QA couple...\")\nfor output in tqdm(outputs):\n    evaluations = {\n        \"groundedness\": call_llm(\n            llm_client,\n            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n        ),\n        \"relevance\": call_llm(\n            llm_client,\n            question_relevance_critique_prompt.format(question=output[\"question\"]),\n        ),\n        \"standalone\": call_llm(\n            llm_client,\n            question_standalone_critique_prompt.format(question=output[\"question\"]),\n        ),\n    }\n    try:\n        for criterion, evaluation in evaluations.items():\n            score, eval = (\n                int(evaluation.split(\"Total rating: \")[-1].strip()),\n                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n            )\n            output.update(\n                {\n                    f\"{criterion}_score\": score,\n                    f\"{criterion}_eval\": eval,\n                }\n            )\n    except Exception as e:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:46.755210Z","iopub.execute_input":"2024-11-17T19:46:46.755708Z","iopub.status.idle":"2024-11-17T19:48:10.193088Z","shell.execute_reply.started":"2024-11-17T19:46:46.755665Z","shell.execute_reply":"2024-11-17T19:48:10.191891Z"}},"outputs":[{"name":"stdout","text":"Generating critique for each QA couple...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df35d3b48a024b56a0811a5158902a06"}},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# Method for printing the kind of messy outputs from the requests above\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", None)\n\ngenerated_questions = pd.DataFrame.from_dict(outputs)\n\nprint(\"Evaluation dataset before filtering:\")\ndisplay(\n    generated_questions[\n        [\n            \"question\",\n            \"answer\",\n            \"groundedness_score\",\n            \"relevance_score\",\n            \"standalone_score\",\n        ]\n    ]\n)\ngenerated_questions = generated_questions.loc[\n    (generated_questions[\"groundedness_score\"] >= 4)\n    & (generated_questions[\"relevance_score\"] >= 4)\n    & (generated_questions[\"standalone_score\"] >= 4)\n]\nprint(\"============================================\")\nprint(\"Final evaluation dataset:\")\ndisplay(\n    generated_questions[\n        [\n            \"question\",\n            \"answer\",\n            \"groundedness_score\",\n            \"relevance_score\",\n            \"standalone_score\",\n        ]\n    ]\n)\n\neval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:48:10.232576Z","iopub.execute_input":"2024-11-17T19:48:10.232926Z","iopub.status.idle":"2024-11-17T19:48:10.391893Z","shell.execute_reply.started":"2024-11-17T19:48:10.232887Z","shell.execute_reply":"2024-11-17T19:48:10.390200Z"}},"outputs":[{"name":"stdout","text":"Evaluation dataset before filtering:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[73], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(outputs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation dataset before filtering:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m display(\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mgenerated_questions\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroundedness_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevance_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstandalone_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m generated_questions\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m     21\u001b[0m     (generated_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m&\u001b[39m (generated_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m&\u001b[39m (generated_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: \"['groundedness_score', 'relevance_score', 'standalone_score'] not in index\""],"ename":"KeyError","evalue":"\"['groundedness_score', 'relevance_score', 'standalone_score'] not in index\"","output_type":"error"}],"execution_count":73},{"cell_type":"code","source":"#eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:40.488098Z","iopub.status.idle":"2024-11-17T19:46:40.488639Z","shell.execute_reply.started":"2024-11-17T19:46:40.488369Z","shell.execute_reply":"2024-11-17T19:46:40.488397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#eval_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:46:40.490945Z","iopub.status.idle":"2024-11-17T19:46:40.492286Z","shell.execute_reply.started":"2024-11-17T19:46:40.491936Z","shell.execute_reply":"2024-11-17T19:46:40.491971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### RAG from Scratch","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n    \"\"\"\n    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:04:32.860035Z","iopub.execute_input":"2024-11-17T20:04:32.860546Z","iopub.status.idle":"2024-11-17T20:04:37.410570Z","shell.execute_reply.started":"2024-11-17T20:04:32.860501Z","shell.execute_reply":"2024-11-17T20:04:37.409325Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nimport os\n\n\ndef load_embeddings(\n    langchain_docs: List[Document],\n    chunk_size: int,\n    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n) -> FAISS:\n    \"\"\"\n    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n\n    Args:\n        langchain_docs: list of documents\n        chunk_size: size of the chunks to split the documents into\n        embedding_model_name: name of the embedding model to use\n\n    Returns:\n        FAISS index\n    \"\"\"\n    # load embedding_model\n    embedding_model = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        multi_process=True,\n        model_kwargs={\"device\": \"cuda\"},\n        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n    )\n\n    # Check if embeddings already exist on disk\n    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n    index_folder_path = f\"./data/indexes/{index_name}/\"\n    if os.path.isdir(index_folder_path):\n        return FAISS.load_local(\n            index_folder_path,\n            embedding_model,\n            distance_strategy=DistanceStrategy.COSINE,\n        )\n\n    else:\n        print(\"Index not found, generating it...\")\n        docs_processed = split_documents(\n            chunk_size,\n            langchain_docs,\n            embedding_model_name,\n        )\n        knowledge_index = FAISS.from_documents(\n            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n        )\n        knowledge_index.save_local(index_folder_path)\n        return knowledge_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.095867Z","iopub.status.idle":"2024-11-17T19:39:45.096504Z","shell.execute_reply.started":"2024-11-17T19:39:45.096166Z","shell.execute_reply":"2024-11-17T19:39:45.096197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"\n<|system|>\nUsing the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\n</s>\n<|assistant|>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.098993Z","iopub.status.idle":"2024-11-17T19:39:45.099468Z","shell.execute_reply.started":"2024-11-17T19:39:45.099250Z","shell.execute_reply":"2024-11-17T19:39:45.099274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceHub\n\nrepo_id = \"HuggingFaceH4/zephyr-7b-beta\"\nREADER_MODEL_NAME = \"zephyr-7b-beta\"\nHF_API_TOKEN = HF_TOKEN\n\nREADER_LLM = HuggingFaceHub(\n    repo_id=repo_id,\n    task=\"text-generation\",\n    huggingfacehub_api_token=HF_API_TOKEN,\n    model_kwargs={\n        \"max_new_tokens\": 512,\n        \"top_k\": 30,\n        \"temperature\": 0.1,\n        \"repetition_penalty\": 1.03,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.100843Z","iopub.status.idle":"2024-11-17T19:39:45.101321Z","shell.execute_reply.started":"2024-11-17T19:39:45.101069Z","shell.execute_reply":"2024-11-17T19:39:45.101091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ragatouille import RAGPretrainedModel\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain_core.language_models.llms import LLM\n\n\ndef answer_with_rag(\n    question: str,\n    llm: LLM,\n    knowledge_index: VectorStore,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 7,\n) -> Tuple[str, List[Document]]:\n    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n    # Gather documents with retriever\n    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n\n    # Optionally rerank results\n    if reranker:\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n    relevant_docs = relevant_docs[:num_docs_final]\n\n    # Build the final prompt\n    context = \"\\nExtracted documents:\\n\"\n    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n\n    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n\n    # Redact an answer\n    answer = llm(final_prompt)\n\n    return answer, relevant_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.103132Z","iopub.status.idle":"2024-11-17T19:39:45.103623Z","shell.execute_reply.started":"2024-11-17T19:39:45.103388Z","shell.execute_reply":"2024-11-17T19:39:45.103411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.language_models import BaseChatModel\n\n\ndef run_rag_tests(\n    eval_dataset: datasets.Dataset,\n    llm,\n    knowledge_index: VectorStore,\n    output_file: str,\n    reranker: Optional[RAGPretrainedModel] = None,\n    verbose: Optional[bool] = True,\n    test_settings: Optional[str] = None,  # To document the test settings used\n):\n    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n    try:  # load previous generations if they exist\n        with open(output_file, \"r\") as f:\n            outputs = json.load(f)\n    except:\n        outputs = []\n\n    for example in tqdm(eval_dataset):\n        question = example[\"question\"]\n        if question in [output[\"question\"] for output in outputs]:\n            continue\n\n        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n        if verbose:\n            print(\"=======================================================\")\n            print(f\"Question: {question}\")\n            print(f\"Answer: {answer}\")\n            print(f'True answer: {example[\"answer\"]}')\n        result = {\n            \"question\": question,\n            \"true_answer\": example[\"answer\"],\n            \"source_doc\": example[\"source_doc\"],\n            \"generated_answer\": answer,\n            \"retrieved_docs\": [doc for doc in relevant_docs],\n        }\n        if test_settings:\n            result[\"test_settings\"] = test_settings\n        outputs.append(result)\n\n        with open(output_file, \"w\") as f:\n            json.dump(outputs, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.105505Z","iopub.status.idle":"2024-11-17T19:39:45.105979Z","shell.execute_reply.started":"2024-11-17T19:39:45.105753Z","shell.execute_reply":"2024-11-17T19:39:45.105776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EVALUATION_PROMPT = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n\n###The instruction to evaluate:\n{instruction}\n\n###Response to evaluate:\n{response}\n\n###Reference Answer (Score 5):\n{reference_answer}\n\n###Score Rubrics:\n[Is the response correct, accurate, and factual based on the reference answer?]\nScore 1: The response is completely incorrect, inaccurate, and/or not factual.\nScore 2: The response is mostly incorrect, inaccurate, and/or not factual.\nScore 3: The response is somewhat correct, accurate, and/or factual.\nScore 4: The response is mostly correct, accurate, and factual.\nScore 5: The response is completely correct, accurate, and factual.\n\n###Feedback:\"\"\"\n\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import SystemMessage\n\n\nevaluation_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=\"You are a fair evaluator language model.\"),\n        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.108143Z","iopub.status.idle":"2024-11-17T19:39:45.108615Z","shell.execute_reply.started":"2024-11-17T19:39:45.108397Z","shell.execute_reply":"2024-11-17T19:39:45.108419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n\nOPENAI_API_KEY = \"\"\n\neval_chat_model = HuggingFaceEndpoint(\n    model=\"prometheus-eval/prometheus-13b-v1.0\",\n    task='text-generation'\n    temperature=0, \n)\nevaluator_name = \"prometheus\"\n\n\ndef evaluate_answers(\n    answer_path: str,\n    eval_chat_model,\n    evaluator_name: str,\n    evaluation_prompt_template: ChatPromptTemplate,\n) -> None:\n    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n    answers = []\n    if os.path.isfile(answer_path):  # load previous generations if they exist\n        answers = json.load(open(answer_path, \"r\"))\n\n    for experiment in tqdm(answers):\n        if f\"eval_score_{evaluator_name}\" in experiment:\n            continue\n\n        eval_prompt = evaluation_prompt_template.format_messages(\n            instruction=experiment[\"question\"],\n            response=experiment[\"generated_answer\"],\n            reference_answer=experiment[\"true_answer\"],\n        )\n        eval_result = eval_chat_model.invoke(eval_prompt)\n        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n        experiment[f\"eval_score_{evaluator_name}\"] = score\n        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n\n        with open(answer_path, \"w\") as f:\n            json.dump(answers, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.110563Z","iopub.status.idle":"2024-11-17T19:39:45.111334Z","shell.execute_reply.started":"2024-11-17T19:39:45.111058Z","shell.execute_reply":"2024-11-17T19:39:45.111083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RAW_KNOWLEDGE_BASE = documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.113204Z","iopub.status.idle":"2024-11-17T19:39:45.113689Z","shell.execute_reply.started":"2024-11-17T19:39:45.113423Z","shell.execute_reply":"2024-11-17T19:39:45.113444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(\"./output\"):\n    os.mkdir(\"./output\")\n\nfor chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n        for rerank in [True, False]:\n            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n            output_file_name = f\"./output/rag_{settings_name}.json\"\n\n            print(f\"Running evaluation for {settings_name}:\")\n\n            print(\"Loading knowledge base embeddings...\")\n            knowledge_index = load_embeddings(\n                RAW_KNOWLEDGE_BASE,\n                chunk_size=chunk_size,\n                embedding_model_name=embeddings,\n            )\n\n            print(\"Running RAG...\")\n            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n            run_rag_tests(\n                eval_dataset=eval_dataset,\n                llm=READER_LLM,\n                knowledge_index=knowledge_index,\n                output_file=output_file_name,\n                reranker=reranker,\n                verbose=False,\n                test_settings=settings_name,\n            )\n\n            print(\"Running evaluation...\")\n            evaluate_answers(\n                output_file_name,\n                eval_chat_model,\n                evaluator_name,\n                evaluation_prompt_template,\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:39:45.115363Z","iopub.status.idle":"2024-11-17T19:39:45.115825Z","shell.execute_reply.started":"2024-11-17T19:39:45.115593Z","shell.execute_reply":"2024-11-17T19:39:45.115620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\n\noutputs = []\nfor file in glob.glob(\"./output/*.json\"):\n    output = pd.DataFrame(json.load(open(file, \"r\")))\n    output[\"settings\"] = file\n    outputs.append(output)\nresult = pd.concat(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\nresult[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\naverage_scores.sort_values()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}