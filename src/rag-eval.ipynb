{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9879418,"sourceType":"datasetVersion","datasetId":5994754}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Pipeline Evaluation\n- Create LLM generated set of Q&A about a document.\n- Use LLM critique/quality control these Q&A pairs.\n- Run another LLM on the pipeline with these Q&A pairs and study output to validate.","metadata":{}},{"cell_type":"code","source":"!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl pandas datasets langchain-community ragatouille langchain-huggingface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:46:47.428627Z","iopub.execute_input":"2024-11-19T03:46:47.429861Z","iopub.status.idle":"2024-11-19T03:47:21.009359Z","shell.execute_reply.started":"2024-11-19T03:46:47.429795Z","shell.execute_reply":"2024-11-19T03:47:21.008043Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:47:21.011962Z","iopub.execute_input":"2024-11-19T03:47:21.012361Z","iopub.status.idle":"2024-11-19T03:47:21.052107Z","shell.execute_reply.started":"2024-11-19T03:47:21.012320Z","shell.execute_reply":"2024-11-19T03:47:21.050968Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Optional, List, Tuple\nimport json\nimport datasets\nimport os\nimport re\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:47:21.053504Z","iopub.execute_input":"2024-11-19T03:47:21.053805Z","iopub.status.idle":"2024-11-19T03:47:25.477926Z","shell.execute_reply.started":"2024-11-19T03:47:21.053775Z","shell.execute_reply":"2024-11-19T03:47:25.476709Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n#HF_TOKEN = <secret key>\n#OPENAI_API_KEY = <secret key>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n#notebook_login()\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:47:25.479774Z","iopub.execute_input":"2024-11-19T03:47:25.480311Z","iopub.status.idle":"2024-11-19T03:47:25.642852Z","shell.execute_reply.started":"2024-11-19T03:47:25.480276Z","shell.execute_reply":"2024-11-19T03:47:25.641665Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Sample Data for Evaluating RAG\n- Import and preprocess a document","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n#from langchain.docstore.document import Document\nfrom langchain.schema import Document\n\n# Function to write outputs to file\ndef write_output_to_file(output, filename):\n    # Ensure the output directory exists\n    out_dir = \"/kaggle/working/\"\n    #os.makedirs(out_dir, exist_ok=True)\n\n    # Define the full file path\n    file_path = os.path.join(out_dir, filename)\n    \n    # Write the output to the file\n    with open(file_path, \"w\") as file:\n        file.write(str(output))\n\n# Function to clean text (to remove unwanted line breaks within sentences)\ndef clean_text(text):\n    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n\n# Function to load documents based on file type\ndef load_documents(file_path):\n    _, file_extension = os.path.splitext(file_path)\n\n    if file_extension.lower() == '.pdf':\n        loader = PyPDFLoader(file_path)\n        print(\"Loading PDF document...\")\n    elif file_extension.lower() == '.md':\n        loader = UnstructuredMarkdownLoader(file_path)\n        print(\"Loading Markdown document...\")\n    elif file_extension.lower() == '.html':\n        loader = UnstructuredHTMLLoader(file_path)\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n    \n    documents = loader.load()\n    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:47:25.717378Z","iopub.execute_input":"2024-11-19T03:47:25.717778Z","iopub.status.idle":"2024-11-19T03:47:26.866384Z","shell.execute_reply.started":"2024-11-19T03:47:25.717740Z","shell.execute_reply":"2024-11-19T03:47:26.865331Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the document and questions\nfile_path = \"/kaggle/input/course-bot-data/bain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\ndocuments = load_documents(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:47:31.655586Z","iopub.execute_input":"2024-11-19T03:47:31.656008Z","iopub.status.idle":"2024-11-19T03:47:32.341847Z","shell.execute_reply.started":"2024-11-19T03:47:31.655969Z","shell.execute_reply":"2024-11-19T03:47:32.340736Z"}},"outputs":[{"name":"stdout","text":"Loading PDF document...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Set some params\nCHUNK_SIZE = 2000\nCHUNK_OVERLAP = 200\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\nsplit_docs = text_splitter.split_documents(documents)\n\n# Add to list for later use\ndocs_processed = []\nfor doc in documents:\n    docs_processed += text_splitter.split_documents([doc])\n\n# Write the chunks to file for manual study\ndocs = []\nfor i, doc in enumerate(split_docs):\n    output_str = f\"Chunk {i + 1}:\\n{doc.page_content}\\n\"\n    docs.append(output_str)\n\nwrite_output_to_file('\\n'.join(docs), 'chunks.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:50:02.187075Z","iopub.execute_input":"2024-11-19T03:50:02.187550Z","iopub.status.idle":"2024-11-19T03:50:02.236424Z","shell.execute_reply.started":"2024-11-19T03:50:02.187503Z","shell.execute_reply":"2024-11-19T03:50:02.235192Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Create Dataset of Q&A Pairs\n- Build synthetic dataset for testing the RAG pipeline\n- Mistral good for this see https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1","metadata":{}},{"cell_type":"code","source":"# Define LLM to generate synthetic Q&A pairs\nfrom huggingface_hub import InferenceClient\n\nqa_creation_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\nllm_client = InferenceClient(model=qa_creation_model, timeout=120,)\n\ndef call_llm(inference_client: InferenceClient, prompt: str):\n    response = inference_client.post(\n        json={\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 1000},\"task\": \"text-generation\",},\n    )\n    return json.loads(response.decode())[0][\"generated_text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:50:38.966163Z","iopub.execute_input":"2024-11-19T03:50:38.966548Z","iopub.status.idle":"2024-11-19T03:50:39.007520Z","shell.execute_reply.started":"2024-11-19T03:50:38.966515Z","shell.execute_reply":"2024-11-19T03:50:39.006464Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"Below, we write a \"factoid\" question prompt (see https://www-cs-faculty.stanford.edu/people/mengqiu/publication/LSII-LitReview.pdf) to generate synthetic data. Such questions ask for a specific fact type answer that is concise. Straightforward task for LLM and reasonable for syllabus over reasoning/advanced analysis.","metadata":{}},{"cell_type":"code","source":"qa_creation_prompt = \"\"\"\n### Instructions\nYour task is to generate a factoid question and its corresponding answer based on the provided context below.\nThe factoid question should\n1. Be answerable with a specific, concise piece of factual information from the context.\n2. Be written in a natural, user-friendly style, similar to what users might input in a search engine.\n3. Avoid mentioning terms like \"context,\" \"passage,\" or \"according to the text.\"\n\nThe answer to the factoid questions should\n1. Be short, precise, and derived directly from the context.\n2. Avoid adding any information that is not explicitly present in the context.\n\n### Formatting\nProvide your response exactly as follows:\n\nOutput:::\nFactoid question: (insert your factoid question here)\nAnswer: (insert your answer to the factoid question here)\n\n### Provided Context\nBelow is the context upon which to base the factoid question and its corresponding answer\n\nContext: {context}\\n\nOutput:::\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:50:41.702568Z","iopub.execute_input":"2024-11-19T03:50:41.702970Z","iopub.status.idle":"2024-11-19T03:50:41.742334Z","shell.execute_reply.started":"2024-11-19T03:50:41.702933Z","shell.execute_reply":"2024-11-19T03:50:41.741006Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import random\nimport time\nN_GENERATIONS = 3  # We intentionally generate only 10 QA couples here for cost and time considerations\n\nprint(f\"Generating {N_GENERATIONS} QA couples...\")\n\noutputs = []\nfor sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n    # Generate QA couple\n    time.sleep(1)\n    output_QA_couple = call_llm(llm_client, qa_creation_prompt.format(context=sampled_context.page_content))\n    try:\n        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n        answer = output_QA_couple.split(\"Answer: \")[-1]\n        assert len(answer) < 300, \"Answer is too long\"\n        outputs.append(\n            {\n                \"context\": sampled_context.page_content,\n                \"question\": question,\n                \"answer\": answer,\n                \"source_doc\": sampled_context.metadata[\"source\"],\n            }\n        )\n    except:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:51:21.287897Z","iopub.execute_input":"2024-11-19T03:51:21.289079Z","iopub.status.idle":"2024-11-19T03:51:28.776221Z","shell.execute_reply.started":"2024-11-19T03:51:21.289037Z","shell.execute_reply":"2024-11-19T03:51:28.774892Z"}},"outputs":[{"name":"stdout","text":"Generating 3 QA couples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4e7398565c4b96a98a752942cba8a4"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Save outputs to readable csv file so we can read manually\noutputs_df = pd.DataFrame(outputs)\noutputs_df.to_csv('generated_qa.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:51:28.814090Z","iopub.execute_input":"2024-11-19T03:51:28.814519Z","iopub.status.idle":"2024-11-19T03:51:28.878719Z","shell.execute_reply.started":"2024-11-19T03:51:28.814472Z","shell.execute_reply":"2024-11-19T03:51:28.877516Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Test one of the questions\nprint(f'Question: {outputs_df.iloc[0,:].question}')\nprint(f'Answer: {outputs_df.iloc[0,:].answer}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:57:47.952644Z","iopub.execute_input":"2024-11-19T03:57:47.953543Z","iopub.status.idle":"2024-11-19T03:57:48.009562Z","shell.execute_reply.started":"2024-11-19T03:57:47.953490Z","shell.execute_reply":"2024-11-19T03:57:48.008286Z"}},"outputs":[{"name":"stdout","text":"Question: What is the primary textbook for the course?\n\nAnswer: The primary textbook for the course is Physics: Principles with Applications, 7th edition, by Douglas C. Giancoli.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Build Critique Models\n- Establish some evaluation metrics for Q&A set, create prompts\n- Use LLM to evaluate the Q&A and create scores\n- Filter our Q&A set quality based on those scores.","metadata":{}},{"cell_type":"markdown","source":"#### Metrics for Critiquing Questions\nRef. https://docs.ragas.io/en/latest/concepts/metrics/index.html \n- Groundedness: can the question be answered from the given context?\n- Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n- Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n- Faithfulness: number of claims in the generated answer that can be inferred from given context / total number of claims in generated answer\n-   Set of claims from generated answer identified\n-   Each claim cross checked within the context.\n- https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/","metadata":{}},{"cell_type":"code","source":"faithfulness_prompt = \"\"\"\nYour task is to evaluate the **faithfulness** of a question based on the provided context.\n\n### Faithfulness Definition:\nFaithfulness is defined as the proportion of claims in the generated answer that can be directly inferred from the context. Specifically:\n- **Faithful Claims**: Claims in the answer that are explicitly stated or logically inferable from the given context.\n- **Unfaithful Claims**: Claims in the answer that are not supported by the context, including hallucinated or extraneous information.\n\nThe faithfulness score is calculated as:\n`Faithfulness Score = (Number of Faithful Claims) / (Total Number of Claims in the Answer)`\n\n### Instructions:\n1. Carefully review the provided context, question, and generated answer.\n2. Identify all individual claims made in the generated answer.\n3. For each claim, determine if it is **faithful** (supported by the context) or **unfaithful** (unsupported or hallucinated).\n4. Compute the faithfulness score and provide a brief explanation for the score.\n\n### Format:\nProvide your response in the following format:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\n### Provided Question and Context\nNow here are the question and context.\n\nQuestion: {question}\\n\nContext: {context}\\n\nAnswer::: \n\n\"\"\"\ngroundedness_prompt = \"\"\"\n### Instructions\nYou will be given a context and a question.\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here are the question and context.\n\nQuestion: {question}\\n\nContext: {context}\\n\nAnswer::: \"\"\"\n\nrelevance_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"\n\nstandalone_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how context-independant this question is.\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\nFor instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T04:20:54.701852Z","iopub.execute_input":"2024-11-19T04:20:54.702806Z","iopub.status.idle":"2024-11-19T04:20:54.745023Z","shell.execute_reply.started":"2024-11-19T04:20:54.702766Z","shell.execute_reply":"2024-11-19T04:20:54.744032Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(\"Generating critique for each QA couple...\")\n# Loop over outputs df and grab the scores\nfor output in tqdm(outputs):\n    evaluations = {\n        \"groundedness\": call_llm(llm_client,groundedness_prompt.format(context=output[\"context\"], question=output[\"question\"])),\n        \"relevance\": call_llm(llm_client,relevance_prompt.format(question=output[\"question\"])),\n        \"standalone\": call_llm(llm_client,standalone_prompt.format(question=output[\"question\"])),\n        \"faithfulness\": call_llm(llm_client, faithfulness_prompt.format(context=output['context'], question=output['question'])),\n    }\n    try:\n        # Loop over each critique criterian and evaluation rating splitting on output string\n        for criterion, evaluation in evaluations.items():\n            score, eval = (\n                int(evaluation.split(\"Total rating: \")[-1].strip()),\n                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n            )\n            output.update({f\"{criterion}_score\": score,\n                           f\"{criterion}_eval\": eval,})\n    except Exception as e:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T04:35:52.878463Z","iopub.execute_input":"2024-11-19T04:35:52.879166Z","iopub.status.idle":"2024-11-19T04:36:53.618695Z","shell.execute_reply.started":"2024-11-19T04:35:52.879085Z","shell.execute_reply":"2024-11-19T04:36:53.617528Z"}},"outputs":[{"name":"stdout","text":"Generating critique for each QA couple...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4308753d110b4d4e8d70b6984ab57135"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# Filter q&A pairs based on critique scores and log\npd.set_option(\"display.max_colwidth\", None)\n\ngenerated_questions = pd.DataFrame.from_dict(outputs)\n\n# Save before filtering\n#generated_questions.to_csv('generated_qa_critique_no_filter.csv', index=False)\n\n# Preview the dataset\nprint(\"Evaluation dataset before filtering:\")\ndisplay(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\",\"faithfulness_score\"]].iloc[0:5,:])\n\n# Filter to make sure relatively relevant responses\ngenerated_questions = generated_questions.loc[\n    (generated_questions[\"groundedness_score\"] >= 1)\n    & (generated_questions[\"relevance_score\"] >= 1)\n    & (generated_questions[\"standalone_score\"] >= 1)\n    & (generated_questions['faithfulness_score'] >= 1)\n]\n\ninvalid_questions = generated_questions.loc[\n    (generated_questions[\"groundedness_score\"] < 1)\n    & (generated_questions[\"relevance_score\"] < 1)\n    & (generated_questions[\"standalone_score\"] < 1)\n    & (generated_questions['faithfulness_score'] < 1)\n]\n\n# Save filtered and killed data\ngenerated_questions.to_csv('generated_qa_critique_filtered.csv', index=False)\ninvalid_questions.to_csv('generated_qa_critique_invalid.csv', index=False)\n\n# Preview filtered dataset\nprint(\"============================================\")\nprint(\"Final evaluation dataset:\")\ndisplay(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\",\"faithfulness_score\"]].iloc[0:5,:])\n\neval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T04:38:03.021307Z","iopub.execute_input":"2024-11-19T04:38:03.021717Z","iopub.status.idle":"2024-11-19T04:38:03.110267Z","shell.execute_reply.started":"2024-11-19T04:38:03.021683Z","shell.execute_reply":"2024-11-19T04:38:03.109142Z"}},"outputs":[{"name":"stdout","text":"Evaluation dataset before filtering:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                                             question  \\\n0                                      What is the primary textbook for the course?\\n   \n1  What is the recommended approach for studying physics to avoid poor test scores?\\n   \n2                                             When is the third exam in the course?\\n   \n\n                                                                                                                                                                                                   answer  \\\n0                                                                                      The primary textbook for the course is Physics: Principles with Applications, 7th edition, by Douglas C. Giancoli.   \n1  Physics demands a daily, systematic, and focused approach to studying, and students should work on problem sets in advance of the deadline instead of cramming the night before an assignment or exam.   \n2                                                                                                                                                        The third exam in the course is on October 29th.   \n\n   groundedness_score  relevance_score  standalone_score  faithfulness_score  \n0                   5              4.0               5.0                 5.0  \n1                   5              1.0               5.0                 4.0  \n2                   5              NaN               NaN                 NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>groundedness_score</th>\n      <th>relevance_score</th>\n      <th>standalone_score</th>\n      <th>faithfulness_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the primary textbook for the course?\\n</td>\n      <td>The primary textbook for the course is Physics: Principles with Applications, 7th edition, by Douglas C. Giancoli.</td>\n      <td>5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the recommended approach for studying physics to avoid poor test scores?\\n</td>\n      <td>Physics demands a daily, systematic, and focused approach to studying, and students should work on problem sets in advance of the deadline instead of cramming the night before an assignment or exam.</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When is the third exam in the course?\\n</td>\n      <td>The third exam in the course is on October 29th.</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"============================================\nFinal evaluation dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                                             question  \\\n0                                      What is the primary textbook for the course?\\n   \n1  What is the recommended approach for studying physics to avoid poor test scores?\\n   \n\n                                                                                                                                                                                                   answer  \\\n0                                                                                      The primary textbook for the course is Physics: Principles with Applications, 7th edition, by Douglas C. Giancoli.   \n1  Physics demands a daily, systematic, and focused approach to studying, and students should work on problem sets in advance of the deadline instead of cramming the night before an assignment or exam.   \n\n   groundedness_score  relevance_score  standalone_score  faithfulness_score  \n0                   5              4.0               5.0                 5.0  \n1                   5              1.0               5.0                 4.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>groundedness_score</th>\n      <th>relevance_score</th>\n      <th>standalone_score</th>\n      <th>faithfulness_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the primary textbook for the course?\\n</td>\n      <td>The primary textbook for the course is Physics: Principles with Applications, 7th edition, by Douglas C. Giancoli.</td>\n      <td>5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the recommended approach for studying physics to avoid poor test scores?\\n</td>\n      <td>Physics demands a daily, systematic, and focused approach to studying, and students should work on problem sets in advance of the deadline instead of cramming the night before an assignment or exam.</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"### Import/Build RAG Pipeline for Evaluation\nCreate methods to:\n1. Split documents and preprocess\n2. Create vector store and fill with embeddings\n3. Build retriever with prompt to get the context and feed to LLM\n4. Run tests to validate","metadata":{}},{"cell_type":"code","source":"# Implement splitting of docs via text_splitter\nfrom transformers import AutoTokenizer\n\ndef split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.035062Z","iopub.execute_input":"2024-11-18T03:44:20.035566Z","iopub.status.idle":"2024-11-18T03:44:20.216061Z","shell.execute_reply.started":"2024-11-18T03:44:20.035516Z","shell.execute_reply":"2024-11-18T03:44:20.214743Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Method to load embeddins and create vectore store\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nimport os\n\ndef load_embeddings(langchain_docs: List[Document], chunk_size: int, embedding_model_name: Optional[str] = \"thenlper/gte-small\",) -> FAISS:\n\n    # load embedding_model\n    embedding_model = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        multi_process=True,\n        #model_kwargs={\"device\": \"cuda\"},\n        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n    )\n\n    # Check if embeddings already exist on disk\n    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n    index_folder_path = f\"./data/indexes/{index_name}/\"\n    if os.path.isdir(index_folder_path):\n        return FAISS.load_local(\n            index_folder_path,\n            embedding_model,\n            distance_strategy=DistanceStrategy.COSINE,\n            allow_dangerous_deserialization=True\n        )\n\n    else:\n        print(\"Index not found, generating it...\")\n        docs_processed = split_documents(\n            chunk_size,\n            langchain_docs,\n            embedding_model_name,\n        )\n        knowledge_index = FAISS.from_documents(\n            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE,\n            allow_dangerous_deserialization=True\n        )\n        knowledge_index.save_local(index_folder_path)\n        return knowledge_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.218311Z","iopub.execute_input":"2024-11-18T03:44:20.218800Z","iopub.status.idle":"2024-11-18T03:44:20.398415Z","shell.execute_reply.started":"2024-11-18T03:44:20.218743Z","shell.execute_reply":"2024-11-18T03:44:20.397056Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"\n<|system|>\nUsing the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\n</s>\n<|assistant|>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.399940Z","iopub.execute_input":"2024-11-18T03:44:20.400389Z","iopub.status.idle":"2024-11-18T03:44:20.574582Z","shell.execute_reply.started":"2024-11-18T03:44:20.400346Z","shell.execute_reply":"2024-11-18T03:44:20.573228Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceHub\n\nrepo_id = \"HuggingFaceH4/zephyr-7b-beta\"\nREADER_MODEL_NAME = \"zephyr-7b-beta\"\nHF_API_TOKEN = HF_TOKEN\n\nREADER_LLM = HuggingFaceHub(\n    repo_id=repo_id,\n    task=\"text-generation\",\n    huggingfacehub_api_token=HF_API_TOKEN,\n    model_kwargs={\n        \"max_new_tokens\": 512,\n        \"top_k\": 30,\n        \"temperature\": 0.1,\n        \"repetition_penalty\": 1.03,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.576228Z","iopub.execute_input":"2024-11-18T03:44:20.576789Z","iopub.status.idle":"2024-11-18T03:44:20.754185Z","shell.execute_reply.started":"2024-11-18T03:44:20.576708Z","shell.execute_reply":"2024-11-18T03:44:20.752920Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"from ragatouille import RAGPretrainedModel\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain_core.language_models.llms import LLM\n\n\ndef answer_with_rag(\n    question: str,\n    llm: LLM,\n    knowledge_index: VectorStore,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 7,\n) -> Tuple[str, List[Document]]:\n    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n    # Gather documents with retriever\n    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n\n    # Optionally rerank results\n    if reranker:\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n    relevant_docs = relevant_docs[:num_docs_final]\n\n    # Build the final prompt\n    context = \"\\nExtracted documents:\\n\"\n    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n\n    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n\n    # Redact an answer\n    answer = llm(final_prompt)\n\n    return answer, relevant_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.755871Z","iopub.execute_input":"2024-11-18T03:44:20.756381Z","iopub.status.idle":"2024-11-18T03:44:20.939272Z","shell.execute_reply.started":"2024-11-18T03:44:20.756334Z","shell.execute_reply":"2024-11-18T03:44:20.938004Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from langchain_core.language_models import BaseChatModel\n\n\ndef run_rag_tests(\n    eval_dataset: datasets.Dataset,\n    llm,\n    knowledge_index: VectorStore,\n    output_file: str,\n    reranker: Optional[RAGPretrainedModel] = None,\n    verbose: Optional[bool] = True,\n    test_settings: Optional[str] = None,  # To document the test settings used\n):\n    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n    try:  # load previous generations if they exist\n        with open(output_file, \"r\") as f:\n            outputs = json.load(f)\n    except:\n        outputs = []\n\n    for example in tqdm(eval_dataset):\n        question = example[\"question\"]\n        if question in [output[\"question\"] for output in outputs]:\n            continue\n\n        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n        if verbose:\n            print(\"=======================================================\")\n            print(f\"Question: {question}\")\n            print(f\"Answer: {answer}\")\n            print(f'True answer: {example[\"answer\"]}')\n        result = {\n            \"question\": question,\n            \"true_answer\": example[\"answer\"],\n            \"source_doc\": example[\"source_doc\"],\n            \"generated_answer\": answer,\n            \"retrieved_docs\": [doc for doc in relevant_docs],\n        }\n        if test_settings:\n            result[\"test_settings\"] = test_settings\n        outputs.append(result)\n\n        with open(output_file, \"w\") as f:\n            json.dump(outputs, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:20.941433Z","iopub.execute_input":"2024-11-18T03:44:20.942411Z","iopub.status.idle":"2024-11-18T03:44:21.125973Z","shell.execute_reply.started":"2024-11-18T03:44:20.942354Z","shell.execute_reply":"2024-11-18T03:44:21.124297Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"EVALUATION_PROMPT = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n\n###The instruction to evaluate:\n{instruction}\n\n###Response to evaluate:\n{response}\n\n###Reference Answer (Score 5):\n{reference_answer}\n\n###Score Rubrics:\n[Is the response correct, accurate, and factual based on the reference answer?]\nScore 1: The response is completely incorrect, inaccurate, and/or not factual.\nScore 2: The response is mostly incorrect, inaccurate, and/or not factual.\nScore 3: The response is somewhat correct, accurate, and/or factual.\nScore 4: The response is mostly correct, accurate, and factual.\nScore 5: The response is completely correct, accurate, and factual.\n\n###Feedback:\"\"\"\n\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import SystemMessage\n\n\nevaluation_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=\"You are a fair evaluator language model.\"),\n        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:44:21.127985Z","iopub.execute_input":"2024-11-18T03:44:21.128538Z","iopub.status.idle":"2024-11-18T03:44:21.306203Z","shell.execute_reply.started":"2024-11-18T03:44:21.128491Z","shell.execute_reply":"2024-11-18T03:44:21.304824Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"#from langchain.chat_models import ChatOpenAI\nfrom langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEndpoint\n\n#llm = HuggingFaceEndpoint(model=\"prometheus-eval/prometheus-7b-v2.0\", task='text-generation', temperature=0)\n#eval_chat_model = ChatHuggingFace(llm=llm)\n#eval_chat_model = READER_LLM\neval_chat_model = ChatOpenAI(model='gpt-4-1106-preview', temperature=0, openai_api_key=OPENAI_API_KEY)\n#eval_chat_model = HuggingFaceEndpoint(\n    #model=\"prometheus-eval/prometheus-13b-v1.0\",\n#    model = 'prometheus-eval/prometheus-7b-v2.0',\n#    task='text-generation',\n#    temperature=0, \n#)\nevaluator_name = \"gpt4\"\n\n\ndef evaluate_answers(\n    answer_path: str,\n    eval_chat_model,\n    evaluator_name: str,\n    evaluation_prompt_template: ChatPromptTemplate,\n) -> None:\n    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n    answers = []\n    if os.path.isfile(answer_path):  # load previous generations if they exist\n        answers = json.load(open(answer_path, \"r\"))\n\n    for experiment in tqdm(answers):\n        if f\"eval_score_{evaluator_name}\" in experiment:\n            continue\n\n        eval_prompt = evaluation_prompt_template.format_messages(\n            instruction=experiment[\"question\"],\n            response=experiment[\"generated_answer\"],\n            reference_answer=experiment[\"true_answer\"],\n        )\n        eval_result = eval_chat_model.invoke(eval_prompt)\n        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n        experiment[f\"eval_score_{evaluator_name}\"] = score\n        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n\n        with open(answer_path, \"w\") as f:\n            json.dump(answers, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:11:10.778721Z","iopub.execute_input":"2024-11-18T05:11:10.779329Z","iopub.status.idle":"2024-11-18T05:11:11.096928Z","shell.execute_reply.started":"2024-11-18T05:11:10.779276Z","shell.execute_reply":"2024-11-18T05:11:11.095632Z"}},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":"### Loop Over Methods to Test + Finetune\n- Loop over hyperparameters and run.\n1. Chunk size\n2. Embedding models and/or critique/evaluation models\n3. Rerank of retrieved contexts or not","metadata":{}},{"cell_type":"code","source":"RAW_KNOWLEDGE_BASE = documents","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(\"./output\"):\n    os.mkdir(\"./output\")\n\nfor chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n        for rerank in [True, False]:\n            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n            output_file_name = f\"./output/rag_{settings_name}.json\"\n\n            print(f\"Running evaluation for {settings_name}:\")\n\n            print(\"Loading knowledge base embeddings...\")\n            knowledge_index = load_embeddings(\n                RAW_KNOWLEDGE_BASE,\n                chunk_size=chunk_size,\n                embedding_model_name=embeddings,\n            )\n\n            print(\"Running RAG...\")\n            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n            run_rag_tests(\n                eval_dataset=eval_dataset,\n                llm=READER_LLM,\n                knowledge_index=knowledge_index,\n                output_file=output_file_name,\n                reranker=reranker,\n                verbose=False,\n                test_settings=settings_name,\n            )\n\n            print(\"Running evaluation...\")\n            evaluate_answers(\n                output_file_name,\n                eval_chat_model,\n                evaluator_name,\n                evaluation_prompt_template,\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:11:17.078386Z","iopub.execute_input":"2024-11-18T05:11:17.078855Z","iopub.status.idle":"2024-11-18T05:11:59.343251Z","shell.execute_reply.started":"2024-11-18T05:11:17.078812Z","shell.execute_reply":"2024-11-18T05:11:59.341964Z"}},"outputs":[{"name":"stdout","text":"Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\nLoading knowledge base embeddings...\nRunning RAG...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966d9385e2de4b76b0c6d44c51d69dfd"}},"metadata":{}},{"name":"stdout","text":"Running evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d897c658384f20affbb72e6841ef3d"}},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EDA the Results","metadata":{}},{"cell_type":"code","source":"import glob\n\noutputs = []\nfor file in glob.glob(\"./output/*.json\"):\n    output = pd.DataFrame(json.load(open(file, \"r\")))\n    output[\"settings\"] = file\n    outputs.append(output)\nresult = pd.concat(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:14:17.617270Z","iopub.execute_input":"2024-11-18T05:14:17.618702Z","iopub.status.idle":"2024-11-18T05:14:17.912441Z","shell.execute_reply.started":"2024-11-18T05:14:17.618614Z","shell.execute_reply":"2024-11-18T05:14:17.910672Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"result[\"eval_score_gpt4\"] = result[\"eval_score_gpt4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\nresult[\"eval_score_gpt4\"] = (result[\"eval_score_gpt4\"] - 1) / 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:14:19.897273Z","iopub.execute_input":"2024-11-18T05:14:19.897789Z","iopub.status.idle":"2024-11-18T05:14:20.171469Z","shell.execute_reply.started":"2024-11-18T05:14:19.897743Z","shell.execute_reply":"2024-11-18T05:14:20.169931Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"average_scores = result.groupby(\"settings\")[\"eval_score_gpt4\"].mean()\naverage_scores.sort_values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:14:21.800465Z","iopub.execute_input":"2024-11-18T05:14:21.800971Z","iopub.status.idle":"2024-11-18T05:14:22.075965Z","shell.execute_reply.started":"2024-11-18T05:14:21.800924Z","shell.execute_reply":"2024-11-18T05:14:22.074137Z"}},"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"settings\n./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json    0.725\nName: eval_score_gpt4, dtype: float64"},"metadata":{}}],"execution_count":94},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(result.eval_score_gpt4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:16:35.493478Z","iopub.execute_input":"2024-11-18T05:16:35.494160Z","iopub.status.idle":"2024-11-18T05:16:36.187419Z","shell.execute_reply.started":"2024-11-18T05:16:35.494091Z","shell.execute_reply":"2024-11-18T05:16:36.186040Z"}},"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"(array([2., 0., 0., 0., 0., 0., 0., 3., 0., 5.]),\n array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}