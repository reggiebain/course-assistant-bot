{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10072105,"sourceType":"datasetVersion","datasetId":5994754}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Pipeline Evaluation\n- Create LLM generated set of Q&A about a document.\n- Use LLM critique/quality control these Q&A pairs.\n- Run another LLM on the pipeline with these Q&A pairs and study output to validate.","metadata":{}},{"cell_type":"code","source":"!pip install -q torch transformers transformers sentence-transformers tqdm openpyxl pandas datasets ragatouille unstructured","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:55:06.704183Z","iopub.execute_input":"2024-12-01T19:55:06.704755Z","iopub.status.idle":"2024-12-01T19:55:19.064754Z","shell.execute_reply.started":"2024-12-01T19:55:06.704690Z","shell.execute_reply":"2024-12-01T19:55:19.063113Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -q langchain langchain-huggingface langchain-community langchain-huggingface langchain-openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:00.735619Z","iopub.execute_input":"2024-12-01T19:56:00.736263Z","iopub.status.idle":"2024-12-01T19:56:15.388975Z","shell.execute_reply.started":"2024-12-01T19:56:00.736224Z","shell.execute_reply":"2024-12-01T19:56:15.387332Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:15.391796Z","iopub.execute_input":"2024-12-01T19:56:15.392216Z","iopub.status.idle":"2024-12-01T19:56:15.472599Z","shell.execute_reply.started":"2024-12-01T19:56:15.392173Z","shell.execute_reply":"2024-12-01T19:56:15.470889Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Optional, List, Tuple\nimport json\nimport datasets\nimport os\nimport re\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:15.474454Z","iopub.execute_input":"2024-12-01T19:56:15.475094Z","iopub.status.idle":"2024-12-01T19:56:15.541097Z","shell.execute_reply.started":"2024-12-01T19:56:15.475031Z","shell.execute_reply":"2024-12-01T19:56:15.539988Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_RAG_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:15.543804Z","iopub.execute_input":"2024-12-01T19:56:15.544327Z","iopub.status.idle":"2024-12-01T19:56:15.942276Z","shell.execute_reply.started":"2024-12-01T19:56:15.544267Z","shell.execute_reply":"2024-12-01T19:56:15.941040Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from huggingface_hub import login\n#notebook_login()\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:15.943930Z","iopub.execute_input":"2024-12-01T19:56:15.944291Z","iopub.status.idle":"2024-12-01T19:56:16.144698Z","shell.execute_reply.started":"2024-12-01T19:56:15.944254Z","shell.execute_reply":"2024-12-01T19:56:16.143456Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Sample Data for Evaluating RAG\n- Import and preprocess a document","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n#from langchain.docstore.document import Document\nfrom langchain.schema import Document\n\n# Function to write outputs to file\ndef write_output_to_file(output, filename):\n    # Ensure the output directory exists\n    out_dir = \"/kaggle/working/\"\n    #os.makedirs(out_dir, exist_ok=True)\n\n    # Define the full file path\n    file_path = os.path.join(out_dir, filename)\n    \n    # Write the output to the file\n    with open(file_path, \"w\") as file:\n        file.write(str(output))\n\n# Function to clean text (to remove unwanted line breaks within sentences)\ndef clean_text(text):\n    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n\n# Function to load documents based on file type\ndef load_documents(file_path):\n    _, file_extension = os.path.splitext(file_path)\n\n    if file_extension.lower() == '.pdf':\n        loader = PyPDFLoader(file_path)\n        print(\"Loading PDF document...\")\n    elif file_extension.lower() == '.md':\n        loader = UnstructuredMarkdownLoader(file_path)\n        print(\"Loading Markdown document...\")\n    elif file_extension.lower() == '.html':\n        loader = UnstructuredHTMLLoader(file_path)\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n    \n    documents = loader.load()\n    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:16.146698Z","iopub.execute_input":"2024-12-01T19:56:16.147185Z","iopub.status.idle":"2024-12-01T19:56:17.588108Z","shell.execute_reply.started":"2024-12-01T19:56:16.147132Z","shell.execute_reply":"2024-12-01T19:56:17.586468Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load the document and questions\n#file_path = \"/kaggle/input/course-bot-data/documents/rbain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\nfile_path = '/kaggle/input/course-bot-data/documents/cbain_syllabus.md'\n#file_path = '/kaggle/input/course-bot-data/cbain_syllabus_111.html'\ndocuments = load_documents(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:17.589284Z","iopub.execute_input":"2024-12-01T19:56:17.589761Z","iopub.status.idle":"2024-12-01T19:56:23.410909Z","shell.execute_reply.started":"2024-12-01T19:56:17.589722Z","shell.execute_reply":"2024-12-01T19:56:23.409551Z"}},"outputs":[{"name":"stdout","text":"Loading Markdown document...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Set some params\nCHUNK_SIZE = 2000\nCHUNK_OVERLAP = 200\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\nsplit_docs = text_splitter.split_documents(documents)\n\n# Add to list for later use\ndocs_processed = []\nfor doc in documents:\n    docs_processed += text_splitter.split_documents([doc])\n\n# Write the chunks to file for manual study\ndocs = []\nfor i, doc in enumerate(split_docs):\n    output_str = f\"Chunk {i + 1}:\\n{doc.page_content}\\n\"\n    docs.append(output_str)\n\nwrite_output_to_file('\\n'.join(docs), 'chunks.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:23.413576Z","iopub.execute_input":"2024-12-01T19:56:23.414370Z","iopub.status.idle":"2024-12-01T19:56:23.540775Z","shell.execute_reply.started":"2024-12-01T19:56:23.414327Z","shell.execute_reply":"2024-12-01T19:56:23.539508Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Create Dataset of Q&A Pairs\n- Build synthetic dataset for testing the RAG pipeline\n- Mistral good for this see https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n- High performing model that has been used for synthetic question generation with 48.6 billion parameters.","metadata":{}},{"cell_type":"code","source":"# Define LLM to generate synthetic Q&A pairs\nfrom huggingface_hub import InferenceClient\n\nqa_creation_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\nllm_client = InferenceClient(model=qa_creation_model, timeout=120,)\n\ndef call_llm(inference_client: InferenceClient, prompt: str):\n    response = inference_client.post(\n        json={\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 1000},\"task\": \"text-generation\",},\n    )\n    return json.loads(response.decode())[0][\"generated_text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:23.542327Z","iopub.execute_input":"2024-12-01T19:56:23.543109Z","iopub.status.idle":"2024-12-01T19:56:23.652986Z","shell.execute_reply.started":"2024-12-01T19:56:23.543055Z","shell.execute_reply":"2024-12-01T19:56:23.651792Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"NOTE: I tried to download the model locally below, but it exceeded maximum allowed disk space on Kaggle. On my local disk, I don't have access to a GPU, which is the best way to make use of it. HuggingFace obviously rate limits these large high performing models, so this part was quite tricky.","metadata":{}},{"cell_type":"code","source":"# Alternate method, download LLM locally\n#from transformers import AutoTokenizer, AutoModelForCausalLM\n\n#qa_creation_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n#tokenizer = AutoTokenizer.from_pretrained(qa_creation_model)\n#model = AutoModelForCausalLM.from_pretrained(qa_creation_model)\n#model.to(\"cuda\")\n\n#def call_local_llm(prompt: str):\n#    inputs = tokenizer(prompt, return_tensors=\"pt\")\n#    outputs = model.generate(inputs.input_ids, max_new_tokens=1000)\n#    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.107455Z","iopub.status.idle":"2024-11-28T05:47:07.109327Z","shell.execute_reply.started":"2024-11-28T05:47:07.108911Z","shell.execute_reply":"2024-11-28T05:47:07.108953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below, we write a \"factoid\" question prompt (see https://www-cs-faculty.stanford.edu/people/mengqiu/publication/LSII-LitReview.pdf) to generate synthetic data. Such questions ask for a specific fact type answer that is concise. Straightforward task for LLM and reasonable for syllabus over reasoning/advanced analysis.","metadata":{}},{"cell_type":"code","source":"qa_creation_prompt = \"\"\"\n### Instructions\nYour task is to generate a factoid question and its corresponding answer based on the provided context below.\nThe factoid question should\n1. Be answerable with a specific, concise piece of factual information from the context.\n2. Be written in a natural, user-friendly style, similar to what users might input in a search engine.\n3. Avoid mentioning terms like \"context,\" \"passage,\" or \"according to the text.\"\n\nThe answer to the factoid questions should\n1. Be short, precise, and derived directly from the context.\n2. Avoid adding any information that is not explicitly present in the context.\n\n### Formatting\nProvide your response exactly as follows:\n\nOutput:::\nFactoid question: (insert your factoid question here)\nAnswer: (insert your answer to the factoid question here)\n\n### Provided Context\nBelow is the context upon which to base the factoid question and its corresponding answer\n\nContext: {context}\\n\nOutput:::\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:27.351739Z","iopub.execute_input":"2024-12-01T19:56:27.352179Z","iopub.status.idle":"2024-12-01T19:56:27.462570Z","shell.execute_reply.started":"2024-12-01T19:56:27.352126Z","shell.execute_reply":"2024-12-01T19:56:27.460827Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import random\nimport time\n#N_GENERATIONS = 50\nN_GENERATIONS = min(30, len(docs_processed))\n\nprint(f\"Generating {N_GENERATIONS} QA couples...\")\n\noutputs = []\nfor sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n    # Generate QA couple\n    time.sleep(3)\n    output_QA_couple = call_llm(llm_client, qa_creation_prompt.format(context=sampled_context.page_content))\n    #output_QA_couple = call_local_llm(qa_creation_prompt.format(context=sampled_context.page_content))\n    try:\n        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n        answer = output_QA_couple.split(\"Answer: \")[-1]\n        assert len(answer) < 300, \"Answer is too long\"\n        outputs.append(\n            {\n                \"context\": sampled_context.page_content,\n                \"question\": question,\n                \"answer\": answer,\n                \"source_doc\": sampled_context.metadata[\"source\"],\n            }\n        )\n    except:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.113747Z","iopub.status.idle":"2024-11-28T05:47:07.114365Z","shell.execute_reply.started":"2024-11-28T05:47:07.114039Z","shell.execute_reply":"2024-11-28T05:47:07.114070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save outputs to readable csv file so we can read manually\noutputs_df = pd.DataFrame(outputs)\noutputs_df.to_csv('generated_qa.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.116139Z","iopub.status.idle":"2024-11-28T05:47:07.116725Z","shell.execute_reply.started":"2024-11-28T05:47:07.116422Z","shell.execute_reply":"2024-11-28T05:47:07.116450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.118176Z","iopub.status.idle":"2024-11-28T05:47:07.118825Z","shell.execute_reply.started":"2024-11-28T05:47:07.118513Z","shell.execute_reply":"2024-11-28T05:47:07.118543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test one of the questions\nprint(f'Question: {outputs_df.iloc[0,:].question}')\nprint(f'Answer: {outputs_df.iloc[0,:].answer}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.120548Z","iopub.status.idle":"2024-11-28T05:47:07.121149Z","shell.execute_reply.started":"2024-11-28T05:47:07.120837Z","shell.execute_reply":"2024-11-28T05:47:07.120867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build Critique Models\n- Establish some evaluation metrics for Q&A set, create prompts\n- Use LLM to evaluate the Q&A and create scores\n- Filter our Q&A set quality based on those scores.","metadata":{}},{"cell_type":"markdown","source":"#### Metrics for Critiquing Questions\nRef. https://docs.ragas.io/en/latest/concepts/metrics/index.html \n- Groundedness: can the question be answered from the given context?\n- Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n- Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n- Faithfulness: number of claims in the generated answer that can be inferred from given context / total number of claims in generated answer\n-   Set of claims from generated answer identified\n-   Each claim cross checked within the context.\n- https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/","metadata":{}},{"cell_type":"code","source":"faithfulness_prompt = \"\"\"\nYour task is to evaluate the **faithfulness** of a question based on the provided context.\n\n### Faithfulness Definition:\nFaithfulness is defined as the proportion of claims in the generated answer that can be directly inferred from the context. Specifically:\n- **Faithful Claims**: Claims in the answer that are explicitly stated or logically inferable from the given context.\n- **Unfaithful Claims**: Claims in the answer that are not supported by the context, including hallucinated or extraneous information.\n\nThe faithfulness score is calculated as:\n`Faithfulness Score = (Number of Faithful Claims) / (Total Number of Claims in the Answer)`\n\n### Instructions:\n1. Carefully review the provided context, question, and generated answer.\n2. Identify all individual claims made in the generated answer.\n3. For each claim, determine if it is **faithful** (supported by the context) or **unfaithful** (unsupported or hallucinated).\n4. Compute the faithfulness score and provide a brief explanation for the score.\n\n### Format:\nProvide your response in the following format:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\n### Provided Question and Context\nNow here are the question and context.\n\nQuestion: {question}\\n\nContext: {context}\\n\nAnswer::: \n\n\"\"\"\ngroundedness_prompt = \"\"\"\n### Instructions\nYou will be given a context and a question.\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here are the question and context.\n\nQuestion: {question}\\n\nContext: {context}\\n\nAnswer::: \"\"\"\n\nrelevance_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"\n\nstandalone_prompt = \"\"\"\nYou will be given a question.\nYour task is to provide a 'total rating' representing how context-independant this question is.\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\nFor instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n\nProvide your answer as follows:\n\nAnswer:::\nEvaluation: (your rationale for the rating, as a text)\nTotal rating: (your rating, as a number between 1 and 5)\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\nNow here is the question.\n\nQuestion: {question}\\n\nAnswer::: \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T19:56:30.802957Z","iopub.execute_input":"2024-12-01T19:56:30.803416Z","iopub.status.idle":"2024-12-01T19:56:30.905613Z","shell.execute_reply.started":"2024-12-01T19:56:30.803357Z","shell.execute_reply":"2024-12-01T19:56:30.904283Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(\"Generating critique for each QA couple...\")\n# Loop over outputs df and grab the scores\nfor output in tqdm(outputs):\n    evaluations = {\n        \"groundedness\": call_llm(llm_client,groundedness_prompt.format(context=output[\"context\"], question=output[\"question\"])),\n        \"relevance\": call_llm(llm_client,relevance_prompt.format(question=output[\"question\"])),\n        \"standalone\": call_llm(llm_client,standalone_prompt.format(question=output[\"question\"])),\n        #\"faithfulness\": call_llm(llm_client, faithfulness_prompt.format(context=output['context'], question=output['question'])),\n    }\n    try:\n        # Loop over each critique criterian and evaluation rating splitting on output string\n        for criterion, evaluation in evaluations.items():\n            score, eval = (\n                int(evaluation.split(\"Total rating: \")[-1].strip()),\n                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n            )\n            output.update({f\"{criterion}_score\": score,\n                           f\"{criterion}_eval\": eval,})\n    except Exception as e:\n        print(f\"{criterion=}\")\n        print(f\"{evaluation=}\")\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.125371Z","iopub.status.idle":"2024-11-28T05:47:07.125940Z","shell.execute_reply.started":"2024-11-28T05:47:07.125649Z","shell.execute_reply":"2024-11-28T05:47:07.125679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter q&A pairs based on critique scores and log\npd.set_option(\"display.max_colwidth\", None)\n\ngenerated_questions = pd.DataFrame.from_dict(outputs)\n\n# Save before filtering\n#generated_questions.to_csv('generated_qa_critique_no_filter.csv', index=False)\n\n# Preview the dataset\nprint(\"Evaluation dataset before filtering:\")\ndisplay(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]].iloc[0:5,:])\n\n# Filter to make sure relatively relevant responses\ngenerated_questions = generated_questions.loc[\n    (generated_questions[\"groundedness_score\"] >= 1)\n    & (generated_questions[\"relevance_score\"] >= 1)\n    & (generated_questions[\"standalone_score\"] >= 1)\n#    & (generated_questions['faithfulness_score'] >= 1)\n]\n\ninvalid_questions = generated_questions.loc[\n    (generated_questions[\"groundedness_score\"] < 1)\n    & (generated_questions[\"relevance_score\"] < 1)\n    & (generated_questions[\"standalone_score\"] < 1)\n #   & (generated_questions['faithfulness_score'] < 1)\n]\n\n# Save filtered and killed data\ngenerated_questions.to_csv('generated_qa_critique_filtered.csv', index=False)\ninvalid_questions.to_csv('generated_qa_critique_invalid.csv', index=False)\n\n# Preview filtered dataset\nprint(\"============================================\")\nprint(\"Final evaluation dataset:\")\ndisplay(generated_questions[[\"question\",\"answer\",\"groundedness_score\",\"relevance_score\",\"standalone_score\"]].iloc[0:5,:])\n\neval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.127427Z","iopub.status.idle":"2024-11-28T05:47:07.128075Z","shell.execute_reply.started":"2024-11-28T05:47:07.127730Z","shell.execute_reply":"2024-11-28T05:47:07.127760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import/Build RAG Pipeline for Evaluation\nCreate methods to:\n1. Split documents and preprocess\n2. Create vector store and fill with embeddings\n3. Build retriever with prompt to get the context and feed to LLM\n4. Run tests to validate","metadata":{}},{"cell_type":"code","source":"# Implement splitting of docs via text_splitter\nfrom transformers import AutoTokenizer\n\ndef split_documents(chunk_size: int, knowledge_base: List[Document],tokenizer_name: str,) -> List[Document]:\n\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.129730Z","iopub.status.idle":"2024-11-28T05:47:07.130306Z","shell.execute_reply.started":"2024-11-28T05:47:07.129994Z","shell.execute_reply":"2024-11-28T05:47:07.130029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next define a method to create or load the embeddings using a FAISS.","metadata":{}},{"cell_type":"code","source":"# Method to load embeddins and create vectore store\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nimport os\n\ndef load_embeddings(langchain_docs, chunk_size, embedding_model_name: Optional[str] = \"thenlper/gte-small\",) -> FAISS:\n\n    embedding_model = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        multi_process=True,\n        #model_kwargs={\"device\": \"cuda\"},\n        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n    )\n\n    # Check if embeddings already exist on disk\n    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n    index_folder_path = f\"./data/indexes/{index_name}/\"\n    if os.path.isdir(index_folder_path):\n        return FAISS.load_local(\n            index_folder_path,\n            embedding_model,\n            distance_strategy=DistanceStrategy.COSINE,\n            allow_dangerous_deserialization=True\n        )\n\n    else:\n        print(\"Index not found, generating it...\")\n        docs_processed = split_documents(\n            chunk_size,\n            langchain_docs,\n            embedding_model_name,\n        )\n        knowledge_index = FAISS.from_documents(\n            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE,\n            #allow_dangerous_deserialization=True\n        )\n        knowledge_index.save_local(index_folder_path)\n        return knowledge_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.131821Z","iopub.status.idle":"2024-11-28T05:47:07.132388Z","shell.execute_reply.started":"2024-11-28T05:47:07.132106Z","shell.execute_reply":"2024-11-28T05:47:07.132136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"\n<|system|>\nUsing the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\n</s>\n<|assistant|>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.133775Z","iopub.status.idle":"2024-11-28T05:47:07.134388Z","shell.execute_reply.started":"2024-11-28T05:47:07.134108Z","shell.execute_reply":"2024-11-28T05:47:07.134138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we'll create a \"reader\" model that will be used to test our RAG pipeline. This should have mostly standard params, but have a very low temperature, since we want it to objectively evaluate results from another LLM.\nModels: \n1. https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n2. https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n3. https://huggingface.co/colbert-ir/colbertv2.0","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceHub\nfrom langchain_huggingface import HuggingFaceEndpoint\n\n# Zephyr 7b from Mistral AI\nrepo_id = \"HuggingFaceH4/zephyr-7b-beta\"\nREADER_MODEL_NAME = \"zephyr-7b-beta\"\n\n# LLama 3.1 8B Instruct from Meta\n#repo_id = 'meta-llama/Llama-3.1-8B-Instruct'\n#READER_MODEL_NAME = 'Llama-3.1-8B-Instruct'\n\n# Declare some parameters for the RAG Q&A LLM\nreader_model_params = {\"max_new_tokens\": 512, \"top_k\": 30,\"temperature\": 0.1,\"repetition_penalty\": 1.03,}\n\nreader_llm = HuggingFaceHub(\n    repo_id=repo_id,\n    task=\"text-generation\", \n    huggingfacehub_api_token=HF_TOKEN,\n    model_kwargs = reader_model_params,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.136363Z","iopub.status.idle":"2024-11-28T05:47:07.136919Z","shell.execute_reply.started":"2024-11-28T05:47:07.136622Z","shell.execute_reply":"2024-11-28T05:47:07.136651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ragatouille import RAGPretrainedModel\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain_core.language_models.llms import LLM\n\ndef answer_with_rag(question: str, llm: LLM, knowledge_index: VectorStore, \n                    reranker: Optional[RAGPretrainedModel] = None, num_retrieved_docs: int = 30,\n                    num_docs_final: int = 7,) -> Tuple[str, List[Document]]:\n    \n    # Gather documents with retriever\n    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n\n    # Optionally rerank results using RAGatoulli ColBERT model\n    if reranker:\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n    relevant_docs = relevant_docs[:num_docs_final]\n\n    # Build the final prompt\n    context = \"\\nExtracted documents:\\n\"\n    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n\n    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n\n    # Redact an answer\n    answer = llm(final_prompt)\n    #answer = llm.invoke(final_prompt)\n\n    return answer, relevant_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.138289Z","iopub.status.idle":"2024-11-28T05:47:07.138845Z","shell.execute_reply.started":"2024-11-28T05:47:07.138548Z","shell.execute_reply":"2024-11-28T05:47:07.138576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.language_models import BaseChatModel\n# Method to run rag on set of questions\ndef run_rag_tests(eval_dataset, llm, knowledge_index, output_file,\n                  reranker: Optional[RAGPretrainedModel] = None, verbose: Optional[bool] = True,\n                  test_settings: Optional[str] = None,):\n    \n    try:  # load previous generations if they exist\n        with open(output_file, \"r\") as f:\n            outputs = json.load(f)\n    except:\n        outputs = []\n\n    # loop over the q&a pairs in the eval dataset \n    for example in tqdm(eval_dataset):\n        question = example[\"question\"]\n        if question in [output[\"question\"] for output in outputs]:\n            continue\n\n        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n        if verbose:\n            print(\"=======================================================\")\n            print(f\"Question: {question}\")\n            print(f\"Answer: {answer}\")\n            print(f'True answer: {example[\"answer\"]}')\n        result = {\n            \"question\": question,\n            \"true_answer\": example[\"answer\"],\n            \"source_doc\": example[\"source_doc\"],\n            \"generated_answer\": answer,\n            \"retrieved_docs\": [doc for doc in relevant_docs],\n        }\n        # Optionally include the details about the model settings throughout the pipeline\n        if test_settings:\n            result[\"test_settings\"] = test_settings\n        outputs.append(result)\n\n        # Print everything to an output json file\n        with open(output_file, \"w\") as f:\n            json.dump(outputs, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.140440Z","iopub.status.idle":"2024-11-28T05:47:07.141124Z","shell.execute_reply.started":"2024-11-28T05:47:07.140698Z","shell.execute_reply":"2024-11-28T05:47:07.140726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EVALUATION_PROMPT = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n\n###The instruction to evaluate:\n{instruction}\n\n###Response to evaluate:\n{response}\n\n###Reference Answer (Score 5):\n{reference_answer}\n\n###Score Rubrics:\n[Is the response correct, accurate, and factual based on the reference answer?]\nScore 1: The response is completely incorrect, inaccurate, and/or not factual.\nScore 2: The response is mostly incorrect, inaccurate, and/or not factual.\nScore 3: The response is somewhat correct, accurate, and/or factual.\nScore 4: The response is mostly correct, accurate, and factual.\nScore 5: The response is completely correct, accurate, and factual.\n\n###Feedback:\"\"\"\n\nfrom langchain.prompts.chat import (ChatPromptTemplate,HumanMessagePromptTemplate,)\nfrom langchain.schema import SystemMessage\n\nevaluation_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=\"You are a fair evaluator language model.\"), \n        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.143205Z","iopub.status.idle":"2024-11-28T05:47:07.143800Z","shell.execute_reply.started":"2024-11-28T05:47:07.143482Z","shell.execute_reply":"2024-11-28T05:47:07.143517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Choose Judge Agent\nNext we declare a judge agent to evaluate the results and a function to evaluate the a&a pairs for various metrics like faithfulness. We'll look at:\n1. OpenAI GPT 4 1106 (Proprietary) - https://community.openai.com/t/gpt-4-1106-preview-vs-gpt-4/588424\n2. FlowAI Judge - https://huggingface.co/flowaicom/Flow-Judge-v0.1-AWQ","metadata":{}},{"cell_type":"code","source":"# Choose a model for judge agent\n#from langchain.chat_models import ChatOpenAI\nfrom langchain_openai import ChatOpenAI\nfrom langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEndpoint\n\n# Uncomment to use best model available GPT4\neval_chat_model = ChatOpenAI(model='gpt-4-1106-preview', temperature=0, openai_api_key=OPENAI_API_KEY)\nevaluator_name = 'gpt4'\n\n# Uncomment to use open source much smaller FlowAI judge\n\n#model_kwargs = {'temperature': 0.1, 'repetition_penalty':1.03}\n#eval_chat_model = ChatHuggingFace(\n#    llm= HuggingFacePipeline.from_model_id(\n#        #endpoint_url = \"https://api-inference.huggingface.co/models/flowaicom/Flow-Judge-v0.1\",\n#        model_id='flowaicom/Flow-Judge-v0.1',\n#        task='text-generation',\n#        model_kwargs=model_kwargs,\n#        #temperature=0.1,\n#        #reptition_penalty=1.03,\n#    )\n#)    \n#evaluator_name = \"flow-judge\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.148346Z","iopub.status.idle":"2024-11-28T05:47:07.148980Z","shell.execute_reply.started":"2024-11-28T05:47:07.148655Z","shell.execute_reply":"2024-11-28T05:47:07.148688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make function to evaluate qa pairs\ndef evaluate_answers(answer_path, eval_chat_model, evaluator_name, \n                     evaluation_prompt_template: ChatPromptTemplate,) -> None:\n    \n    answers = []\n    # Only work on files where answer_path exists (i.e. where the q&a pairs are)\n    if os.path.isfile(answer_path):\n        answers = json.load(open(answer_path, \"r\"))\n\n    for experiment in tqdm(answers):\n        # Move on if score is already recorded\n        if f\"eval_score_{evaluator_name}\" in experiment:\n            continue\n\n        eval_prompt = evaluation_prompt_template.format_messages(\n            instruction=experiment[\"question\"],\n            response=experiment[\"generated_answer\"],\n            reference_answer=experiment[\"true_answer\"],\n        )\n        eval_result = eval_chat_model.invoke(eval_prompt)\n        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n        experiment[f\"eval_score_{evaluator_name}\"] = score\n        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n\n        with open(answer_path, \"w\") as f:\n            json.dump(answers, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.150272Z","iopub.status.idle":"2024-11-28T05:47:07.150828Z","shell.execute_reply.started":"2024-11-28T05:47:07.150543Z","shell.execute_reply":"2024-11-28T05:47:07.150571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loop Over Methods to Test + Finetune\n- Loop over hyperparameters and run.\n1. Chunk size\n2. Embedding models and/or critique/evaluation models\n3. Rerank of retrieved contexts or not","metadata":{}},{"cell_type":"code","source":"RAW_KNOWLEDGE_BASE = documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.152496Z","iopub.status.idle":"2024-11-28T05:47:07.153108Z","shell.execute_reply.started":"2024-11-28T05:47:07.152797Z","shell.execute_reply":"2024-11-28T05:47:07.152828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(\"./output\"):\n    os.mkdir(\"./output\")\n\nfor chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n        for rerank in [True, False]:\n            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n            output_file_name = f\"./output/rag_{settings_name}.json\"\n\n            print(f\"Running evaluation for {settings_name}:\")\n\n            print(\"Loading knowledge base embeddings...\")\n            knowledge_index = load_embeddings(\n                RAW_KNOWLEDGE_BASE,\n                chunk_size=chunk_size,\n                embedding_model_name=embeddings,\n            )\n\n            print(\"Running RAG...\")\n            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n            run_rag_tests(\n                eval_dataset=eval_dataset,\n                llm=reader_llm,\n                knowledge_index=knowledge_index,\n                output_file=output_file_name,\n                reranker=reranker,\n                verbose=False,\n                test_settings=settings_name,\n            )\n\n            print(\"Running evaluation...\")\n            evaluate_answers(\n                output_file_name,\n                eval_chat_model,\n                evaluator_name,\n                evaluation_prompt_template,\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.154531Z","iopub.status.idle":"2024-11-28T05:47:07.155138Z","shell.execute_reply.started":"2024-11-28T05:47:07.154805Z","shell.execute_reply":"2024-11-28T05:47:07.154833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EDA the Results","metadata":{}},{"cell_type":"code","source":"# Use glob to recursively go through all of the json output files we dumped\nimport glob\n\noutputs = []\nfor file in glob.glob(\"./output/*.json\"):\n    output = pd.DataFrame(json.load(open(file, \"r\")))\n    output[\"settings\"] = file\n    outputs.append(output)\n    \n# Make dataframe of output results\nresult = pd.concat(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.157275Z","iopub.status.idle":"2024-11-28T05:47:07.157855Z","shell.execute_reply.started":"2024-11-28T05:47:07.157566Z","shell.execute_reply":"2024-11-28T05:47:07.157596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.160023Z","iopub.status.idle":"2024-11-28T05:47:07.160685Z","shell.execute_reply.started":"2024-11-28T05:47:07.160355Z","shell.execute_reply":"2024-11-28T05:47:07.160386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result[\"eval_score_gpt4\"] = result[\"eval_score_gpt4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\nresult[\"eval_score_gpt4\"] = (result[\"eval_score_gpt4\"] - 1) / 4\nresult","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.162998Z","iopub.status.idle":"2024-11-28T05:47:07.163661Z","shell.execute_reply.started":"2024-11-28T05:47:07.163331Z","shell.execute_reply":"2024-11-28T05:47:07.163364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"average_scores = result.groupby(\"settings\")[\"eval_score_gpt4\"].mean()\naverage_scores.sort_values()\naverage_scores.to_csv('average_scores.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.165698Z","iopub.status.idle":"2024-11-28T05:47:07.166335Z","shell.execute_reply.started":"2024-11-28T05:47:07.165996Z","shell.execute_reply":"2024-11-28T05:47:07.166029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(result.eval_score_gpt4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.168489Z","iopub.status.idle":"2024-11-28T05:47:07.169207Z","shell.execute_reply.started":"2024-11-28T05:47:07.168890Z","shell.execute_reply":"2024-11-28T05:47:07.168922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r 'rag_eval_results.zip' '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:47:07.170931Z","iopub.status.idle":"2024-11-28T05:47:07.171512Z","shell.execute_reply.started":"2024-11-28T05:47:07.171229Z","shell.execute_reply":"2024-11-28T05:47:07.171258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baselines","metadata":{}}]}