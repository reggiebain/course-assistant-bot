{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic QA in LangChain\n",
    "- Embed document\n",
    "- Create vector store\n",
    "- Query vector store with qa chain\n",
    "- Parse results\n",
    "\n",
    "Note: This uses langchain 0.3.7 which is no longer supported but is the latest version on Kaggle. LangChain 1.xxx is significantly different using their new LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:08:21.624257Z",
     "iopub.status.busy": "2024-11-11T03:08:21.623894Z",
     "iopub.status.idle": "2024-11-11T03:08:36.678856Z",
     "shell.execute_reply": "2024-11-11T03:08:36.677306Z",
     "shell.execute_reply.started": "2024-11-11T03:08:21.624214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers langchain sentence-transformers pypdf faiss-cpu langchain-community torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:13:01.512354Z",
     "iopub.status.busy": "2024-11-11T03:13:01.511053Z",
     "iopub.status.idle": "2024-11-11T03:13:01.518099Z",
     "shell.execute_reply": "2024-11-11T03:13:01.516910Z",
     "shell.execute_reply.started": "2024-11-11T03:13:01.512299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/transformers/utils/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     31\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     42\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader  # Assumes both loaders exist\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_to_file(output, filename: str):\n",
    "    # Ensure the output directory exists\n",
    "    out_dir = \"../out/\"\n",
    "    #os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the full file path\n",
    "    file_path = os.path.join(out_dir, filename)\n",
    "    \n",
    "    # Write the output to the file\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(str(output))\n",
    "    \n",
    "    print(f\"Output successfully written to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, Clean, and Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text (to remove unwanted line breaks within sentences)\n",
    "def clean_text(text):\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "# Function to load documents based on file type\n",
    "def load_documents(file_path):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    \n",
    "    if file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        print(\"Loading PDF document...\")\n",
    "    elif file_extension.lower() == '.md':\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        print(\"Loading Markdown document...\")\n",
    "    elif file_extension.lower() == '.html':\n",
    "        loader = UnstructuredHTMLLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a PDF or Markdown file.\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    cleaned_documents = [Document(page_content=clean_text(doc.page_content)) for doc in documents]\n",
    "    return cleaned_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "file_path = \"/kaggle/input/course-bot-data/bain_syllabus.pdf\"  # Change this to the path of your PDF or Markdown file\n",
    "documents = load_documents(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(split_docs)}\")\n",
    "\n",
    "print(\"Sample chunks:\")\n",
    "for i, doc in enumerate(split_docs[:5]):\n",
    "    write_output_to_file(print(f\"Chunk {i + 1}:\\n{doc.page_content}\\n\"), 'chunk.out': str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:14:41.763279Z",
     "iopub.status.busy": "2024-11-11T03:14:41.761813Z",
     "iopub.status.idle": "2024-11-11T03:17:27.385539Z",
     "shell.execute_reply": "2024-11-11T03:17:27.384231Z",
     "shell.execute_reply.started": "2024-11-11T03:14:41.763223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF document...\n",
      "Total chunks created: 22\n",
      "Sample chunks:\n",
      "Chunk 1:\n",
      "I. Course Description 1. Course Summary a. PHY 161/PHYS 215 General Physics I is an algebra-based introduction to mechanics,  thermodynamics, and waves. Topics include motion in one and two dimensions,  Newtonâ€™s laws of motion, equilibrium, work, energy, momentum, rotational motion,  gravity, heat, waves, and sound. Examples from medicine and biology will be  included whenever possible. 2. College Credit Hours (Dual-Enrollment) a. This course is dual enrolled with PHYS 215 General Physics I at Francis Marion  University (FMU) and taught by a GSSM instructor. Students will each have a FMU  transcript with their overall grade earned in this course. Students may earn up to 4  college credit hours depending on their grade and the transfer policies of their  college/university. Refer to the Dual Enrollment FAQ in the Course Catalog for  more information. 3. Learning Outcomes a. Upon completion of this course, students will be able to:  b. Apply the laws of classical Newtonian mechanics\n",
      "\n",
      "Chunk 2:\n",
      "Dual Enrollment FAQ in the Course Catalog for  more information. 3. Learning Outcomes a. Upon completion of this course, students will be able to:  b. Apply the laws of classical Newtonian mechanics (motion, force, energy,  momentum, and gravitation) to solve problems involving static systems, motion  with constant acceleration, and rotational motion.  c. Use techniques of graphical analysis to classify and model physical phenomena and  apply physical meaning to mathematical techniques from algebra, pre-calculus, and  trigonometry. Page 1 of 8 Course PHY 161 - General Physics 1 (Dual Enrolled w/ FMU) Semester Fall 2024 Lecture TuThFr 1-2 | TuThFr 2-3 | TuThFr 3-4 Lab Mo 2-3 | Tu 10-12 | We 2-4 Location C-107 and https://gssm.zoom.us/j/8622923374 Instructor Dr. Reginald Bain | âœ‰ rbain@governors.school ðŸ“ž 843-383-3900 | ðŸ¢ Oï¬ƒce: B166/Zoom (862-292-3374) ðŸ• Oï¬ƒce Hours: By appointment OR MoWe 10-11, 1-2, ThFr 10-11\n",
      "\n",
      "Chunk 3:\n",
      "d. Work collaboratively inside and outside the classroom through activities such as  polling questions, hands-on experiments, guided inquiry exercises, and group  problem-solving assignments.  e. Collect, analyze, and present scientiï¬c data through weekly labs, data-focused  problems, and technical writing tasks.  4. Pedagogy a. This course will help students develop key problem-solving and critical thinking skills  through a student-centered, active approach to learning. The course will incorporate  guided, inquiry-based activities, group problem-solving, and peer-instruction in order to  promote active student engagement. II. Laboratory 1. Learning Outcomes a. Lab oï¬€ers students the chance to gain interactive experience with scientiï¬c principles,  develop their data collection/analysis skills, and work collaboratively with other students  to complete experiments and solve problems as a group.  b. Lab will consist of a mixture of hands-on experiments, inquiry-based group work,\n",
      "\n",
      "Chunk 4:\n",
      "skills, and work collaboratively with other students  to complete experiments and solve problems as a group.  b. Lab will consist of a mixture of hands-on experiments, inquiry-based group work,  individual problem-solving, data analysis, experimental design, and online physics  simulations such as those designed by the PhET collaboration. 2. Assignments a. Students will complete weekly lab assignments that will be written in Google Docs and  utilize Google Sheets extensively. Most labs will be submitted electronically to Canvas but  some will be written by hand. Some activities will require students to work in groups of  3-4 and others will require students to turn in work individually. During most weeks, the  assignment should be completed and submitted during the lab period.  b. Students may be asked to complete a more formal lab write-up or other technical writing  assignment following a speciï¬c rubric (rubric will be posted to Canvas). Such an  assignment will count for 15% of the\n",
      "\n",
      "Chunk 5:\n",
      "may be asked to complete a more formal lab write-up or other technical writing  assignment following a speciï¬c rubric (rubric will be posted to Canvas). Such an  assignment will count for 15% of the total lab grade. III. Required Resources 1. Textbook Resources a. Physics: Principles with Applications, 7th ed., Douglas C. Giancoli â€” Primary text for  course. A physical copy is provided to students.  b. OpenStax College Physics â€” An additional free e-textbook will be provided to students on  Canvas as an additional course resource.  c. Additional Resources â€” Students will be asked to use a wide variety of simulations, watch  online physics videos, and may be assigned additional reading from resources outside the  test above. Such readings and assignments will be provided on Canvas and with each  WebAssign homework assignment. 2. WebAssign Page 2 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7a375b5fe3471692c8f15ccfa41b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What will students use to submit homework assignments?\n",
      "Query embedding (first 5 values): [0.010682711377739906, -0.009580872021615505, -0.0028102046344429255, -0.06568533182144165, -0.029421314597129822]...\n",
      "\n",
      "Retrieved Context 1 (Chunk Index: 5):\n",
      "a. Students will submit HW assignments and complete in-class tests using WebAssign, an  online platform used by many universities that provides students with instant feedback  on problem responses along with helpful tutorials.  b. How to sign up: See â€œWebAssign Registrationâ€ module on Canvas for help with signing up  for a WebAssign account and the class key code. 3. Needed Supplies 1. To the class, students should, at minimum, bringâ€¦  (1) Writing utensils with notebook, printed notes, or tablets/iPad.  (a) Note - Students should NOT use their phone or laptop in class.  (b) Note - Students should NOT wear headphones during class.  2. To the lab, students should bringâ€¦  (1) One person per group should bring a laptop if possible. I have a few classroom  laptops as well. This will be used for Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1.\n",
      "\n",
      "Retrieved Context 2 (Chunk Index: 12):\n",
      "a. Done via WebAssign, roughly 1-2 assignments will be due per week constituting ~10-20  problems/week. Deadlines are always visible online. Students are responsible for checking  WebAssign/Canvas for due dates.  b. Students SHOULD NOT leave these assignments until the last minute and should  individually budget several hours of time to work on problems each week that works with  their schedule. Typical HW deadlines will be Tuesday/Friday evenings.  c. A list of daily reading assignments is posted on Canvas. Students should read the assigned  sections before each class period. 4. Missed/Late Assignments a. Missed/late assignments without an oï¬ƒcial excused absence will receive no credit. If a  student will miss class because of an excused absence, students may request an  extension from the instructor beforehand. They can request this extension directly on  WebAssign. Extensions will only be provided for excused absences. Otherwise, no late  work is accepted.  b. Valid excuses include\n",
      "\n",
      "Retrieved Context 3 (Chunk Index: 6):\n",
      "Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1. Grading Scale a. Numeric grades will be calculated using the weighting shown below. b. In-class participation (answering in-class clicker questions and completing activities in  class and labs in good faith) will count towards the lab/participation grade. Students must  answer every question posed in class using polling/clickers.  c. Any potential quizzes will count towards the HW grade. 2. Keys for success Page 3 of 8 Grade Weighting  20% Homework  20% Lab/Participation  45% Tests (4 x 11.25%)  15% Final Exam GSSM/State Grading  Numerical grades will be on your  state/GSSM high school transcript. Letter  grades are based on the following  10-point Grade Scale  A : 9 0 â€“ 1 0 0  B : 8 0 â€“ 8 9 . 5  C : 7 0 â€“ 7 9 . 5  D : < 7 0  Grades will be regularly updated on  Canvas  S t u d e n t s m u\n",
      "\n",
      "Retrieved Context:\n",
      "Context 1:\n",
      "a. Students will submit HW assignments and complete in-class tests using WebAssign, an  online platform used by many universities that provides students with instant feedback  on problem responses along with helpful tutorials.  b. How to sign up: See â€œWebAssign Registrationâ€ module on Canvas for help with signing up  for a WebAssign account and the class key code. 3. Needed Supplies 1. To the class, students should, at minimum, bringâ€¦  (1) Writing utensils with notebook, printed notes, or tablets/iPad.  (a) Note - Students should NOT use their phone or laptop in class.  (b) Note - Students should NOT wear headphones during class.  2. To the lab, students should bringâ€¦  (1) One person per group should bring a laptop if possible. I have a few classroom  laptops as well. This will be used for Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1.\n",
      "\n",
      "Context 2:\n",
      "a. Done via WebAssign, roughly 1-2 assignments will be due per week constituting ~10-20  problems/week. Deadlines are always visible online. Students are responsible for checking  WebAssign/Canvas for due dates.  b. Students SHOULD NOT leave these assignments until the last minute and should  individually budget several hours of time to work on problems each week that works with  their schedule. Typical HW deadlines will be Tuesday/Friday evenings.  c. A list of daily reading assignments is posted on Canvas. Students should read the assigned  sections before each class period. 4. Missed/Late Assignments a. Missed/late assignments without an oï¬ƒcial excused absence will receive no credit. If a  student will miss class because of an excused absence, students may request an  extension from the instructor beforehand. They can request this extension directly on  WebAssign. Extensions will only be provided for excused absences. Otherwise, no late  work is accepted.  b. Valid excuses include\n",
      "\n",
      "Context 3:\n",
      "Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1. Grading Scale a. Numeric grades will be calculated using the weighting shown below. b. In-class participation (answering in-class clicker questions and completing activities in  class and labs in good faith) will count towards the lab/participation grade. Students must  answer every question posed in class using polling/clickers.  c. Any potential quizzes will count towards the HW grade. 2. Keys for success Page 3 of 8 Grade Weighting  20% Homework  20% Lab/Participation  45% Tests (4 x 11.25%)  15% Final Exam GSSM/State Grading  Numerical grades will be on your  state/GSSM high school transcript. Letter  grades are based on the following  10-point Grade Scale  A : 9 0 â€“ 1 0 0  B : 8 0 â€“ 8 9 . 5  C : 7 0 â€“ 7 9 . 5  D : < 7 0  Grades will be regularly updated on  Canvas  S t u d e n t s m u\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547/1621150229.py:94: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "a. Students will submit HW assignments and complete in-class tests using WebAssign, an  online platform used by many universities that provides students with instant feedback  on problem responses along with helpful tutorials.  b. How to sign up: See â€œWebAssign Registrationâ€ module on Canvas for help with signing up  for a WebAssign account and the class key code. 3. Needed Supplies 1. To the class, students should, at minimum, bringâ€¦  (1) Writing utensils with notebook, printed notes, or tablets/iPad.  (a) Note - Students should NOT use their phone or laptop in class.  (b) Note - Students should NOT wear headphones during class.  2. To the lab, students should bringâ€¦  (1) One person per group should bring a laptop if possible. I have a few classroom  laptops as well. This will be used for Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1.\n",
      "\n",
      "a. Done via WebAssign, roughly 1-2 assignments will be due per week constituting ~10-20  problems/week. Deadlines are always visible online. Students are responsible for checking  WebAssign/Canvas for due dates.  b. Students SHOULD NOT leave these assignments until the last minute and should  individually budget several hours of time to work on problems each week that works with  their schedule. Typical HW deadlines will be Tuesday/Friday evenings.  c. A list of daily reading assignments is posted on Canvas. Students should read the assigned  sections before each class period. 4. Missed/Late Assignments a. Missed/late assignments without an oï¬ƒcial excused absence will receive no credit. If a  student will miss class because of an excused absence, students may request an  extension from the instructor beforehand. They can request this extension directly on  WebAssign. Extensions will only be provided for excused absences. Otherwise, no late  work is accepted.  b. Valid excuses include\n",
      "\n",
      "Google Docs/Google Sheets/LoggerPro  (2) Writing utensil with notebook/something to write on (tablets are ok)  (3) Whenever available, students should read lab instructions in advance IV. Grading 1. Grading Scale a. Numeric grades will be calculated using the weighting shown below. b. In-class participation (answering in-class clicker questions and completing activities in  class and labs in good faith) will count towards the lab/participation grade. Students must  answer every question posed in class using polling/clickers.  c. Any potential quizzes will count towards the HW grade. 2. Keys for success Page 3 of 8 Grade Weighting  20% Homework  20% Lab/Participation  45% Tests (4 x 11.25%)  15% Final Exam GSSM/State Grading  Numerical grades will be on your  state/GSSM high school transcript. Letter  grades are based on the following  10-point Grade Scale  A : 9 0 â€“ 1 0 0  B : 8 0 â€“ 8 9 . 5  C : 7 0 â€“ 7 9 . 5  D : < 7 0  Grades will be regularly updated on  Canvas  S t u d e n t s m u\n",
      "\n",
      "Question: What will students use to submit homework assignments?\n",
      "Helpful Answer: According to the context, students will submit homework assignments using WebAssign, an online platform that provides instant feedback and helpful tutorials. Additionally, students will also complete in-class tests using WebAssign.\n",
      "\n",
      "More Context:\n",
      "\n",
      "* To sign up for a WebAssign account and the class key code, students should see the \"WebAssign Registration\" module on Canvas.\n",
      "* The context mentions that students should NOT use their phone or laptop in class and also should NOT wear headphones during class.\n",
      "* The context also mentions that students\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab/declare tokenizer and model (used transformers below)\n",
    "model_path = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\" # Use with models loaded into Kaggle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare langchian Q&A chain using a hugging face pipeline\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, temperature=0.7)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Set up retrieval-based QA chain with vector store as the retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug function to track the retrieval process\n",
    "def debug_retrieval(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    # Print query embedding to verify unique representation\n",
    "    query_embedding = embeddings.embed_query(question)\n",
    "    print(f\"Query embedding (first 5 values): {query_embedding[:5]}...\\n\")\n",
    "\n",
    "    # Retrieve relevant documents and print context chunks\n",
    "    retrieval_result = retriever.get_relevant_documents(question)\n",
    "    for i, doc in enumerate(retrieval_result):\n",
    "        chunk_index = split_docs.index(doc) if doc in split_docs else -1  # Find the index of the document in split_docs\n",
    "        print(f\"Retrieved Context {i + 1} (Chunk Index: {chunk_index}):\\n{doc.page_content}\\n\")\n",
    "\n",
    "    return retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "chat_history = []\n",
    "question = 'What will students use to submit homework assignments?'\n",
    "\n",
    "# Retrieve the context\n",
    "#retrieval_result = retriever.get_relevant_documents(question)\n",
    "retrieval_result = debug_retrieval(question) # Use debug retrieval to get print statements\n",
    "context = \" \".join([doc.page_content for doc in retrieval_result])\n",
    "\n",
    "# Run answer using the qa chani we declared earlier\n",
    "answer = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "print(\"Answer:\", answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_response(response) -> dict:\n",
    "    # Use regex to find matches for section headers and their contents\n",
    "    matches = re.findall(r'([A-Z][a-zA-Z]*):\\s(.*?)(?=\\n[A-Z]|$)', answer_text, re.DOTALL)\n",
    "\n",
    "    return {title: content.strip() for title, content in matches} \n",
    "\n",
    "parsed_output = parse_response(answer['answer'])\n",
    "for section, text in parsed_output.items():\n",
    "    print(f\"{section}: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with retrieval QA class rather than the conversational QA one. Note that both are deprecated. But this is what Kaggle will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:17:27.388148Z",
     "iopub.status.busy": "2024-11-11T03:17:27.387723Z",
     "iopub.status.idle": "2024-11-11T03:19:09.069510Z",
     "shell.execute_reply": "2024-11-11T03:19:09.067837Z",
     "shell.execute_reply.started": "2024-11-11T03:17:27.388104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# Set up retrieval-based QA chain\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "#qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,retriever=retriever)\n",
    "# Ask a question\n",
    "chat_history = []\n",
    "question = \"Can I turn in homework late?\"\n",
    "\n",
    "# Retrieve the context\n",
    "retrieval_result = retriever.get_relevant_documents(question)\n",
    "#retrieved_result = debug_retrieval(question)\n",
    "context = \" \".join([doc.page_content for doc in retrieval_result])\n",
    "\n",
    "# Generate the answer\n",
    "response = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each answer from the ConversationalQA chain has several key value pairs:\n",
    "1. question\n",
    "2. chat_history\n",
    "3. answer\n",
    "\n",
    "When using retrieval QA chain the parts are\n",
    "1. query\n",
    "2. result\n",
    "\n",
    "The response is simply two strings. The result string contains several sections including the prompt leading up to Answer, Question, Helpful Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the cosine similarity of contexts for a question we know works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Get the embedding of the query\n",
    "query = \"What will students use to submit homework assignments?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Retrieve all chunk embeddings from the vector store (FAISS in this case)\n",
    "# Extracting all chunks and embeddings for manual similarity calculation\n",
    "all_chunk_embeddings = vector_store.index.reconstruct_n(0, len(split_docs))\n",
    "\n",
    "# Calculate cosine similarity between the query embedding and each chunk\n",
    "similarity_scores = cosine_similarity([query_embedding], all_chunk_embeddings).flatten()\n",
    "\n",
    "# Pair each chunk with its similarity score\n",
    "chunk_scores = list(zip(split_docs, similarity_scores))\n",
    "\n",
    "# Sort chunks by similarity score in descending order\n",
    "sorted_chunk_scores = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 most similar chunks\n",
    "print(\"Top 5 most relevant chunks:\")\n",
    "for i, (doc_chunk, score) in enumerate(sorted_chunk_scores[:5]):\n",
    "    print(f\"Chunk {i + 1} - Similarity Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc_chunk.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "questions = ['What topics are included in this course?',\n",
    "             'Through what university is this course dual-enrolled?',\n",
    "             'Please list the learning outcomes for the course.',\n",
    "             'Will this course incorporate inquiry-based activities?',\n",
    "             'Are there any free e-textbooks provided to students?',\n",
    "             'How will students submit labs?',\n",
    "             \"How big will lab groups be?\"\n",
    "             'What is the primary text for the course?',\n",
    "             'What will students use to submit homework assignments?',\n",
    "             'What supplies do students need to bring to class?',\n",
    "             'What weighting is given to the Final Exam?',\n",
    "             'Please list 3 keys for success.',\n",
    "             'List any prerequisites or co-requisites for the course.',\n",
    "             'Describe the coure lab tardiness policy.',\n",
    "             'Are students allowed to wear headphones?',\n",
    "             'How many homework assignments do students have to complete?',\n",
    "             \"What is the course policy on missed/late assignments?\",\n",
    "             \"Describe how tests/exams are administered.\",\n",
    "             'What is the name of the Director of the Center for Academic Success?',\n",
    "             'If a student engages in plagiarism/cheating, what will happen?',\n",
    "             'List the dates of the exams for the course.',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def debug_retrieval2(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    # Print query embedding to verify unique representation\n",
    "    query_embedding = embeddings.embed_query(question)\n",
    "    print(f\"Query embedding (first 5 values): {query_embedding[:5]}...\\n\")\n",
    "\n",
    "    # Retrieve relevant documents and print context chunks\n",
    "    retrieval_result = retriever.get_relevant_documents(question)\n",
    "    for i, doc in enumerate(retrieval_result):\n",
    "        chunk_index = split_docs.index(doc) if doc in split_docs else -1  # Find the index of the document in split_docs\n",
    "        print(f\"Retrieved Context {i + 1} (Chunk Index: {chunk_index}):\\n{doc.page_content}\\n\")\n",
    "\n",
    "    return retrieval_result, chunk_index\n",
    "\n",
    "\n",
    "def debug_answers(question):\n",
    "    retrieved_result = debug_retrieval(question)\n",
    "    context = \" \".join([doc.page_content for doc in retrieval_result])\n",
    "    print(\"Retrieved Context:\")\n",
    "    for i, doc in enumerate(retrieval_result):\n",
    "        print(f\"Context {i + 1}:\\n{doc.page_content}\\n\")\n",
    "    # Generate the answer\n",
    "    answer = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"Answer:\", answer['answer'])\n",
    "\n",
    "# Set up retrieval-based QA chain\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "chunks_used = []\n",
    "for question in questions:\n",
    "    chunks_used.append(debug_retrieval2(question)[1])\n",
    "    #debug_retrieval2(question)\n",
    "\n",
    "        \n",
    "    \n",
    "plt.hist(chunks_used)\n",
    "plt.title('Histogram of Context Chunks Used')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "- Experiment with chunk size -- smaller means more specific but could miss info\n",
    "- Experiment with chunk overlap\n",
    "- Implement semantic splitting to split chunks on obvious sections. Use nltk/spacy?\n",
    "- Experiment with different vector stores: FAISS (fast) vs. Chroma vs. Weaviate\n",
    "- Experiment with k, number of retrieved documents (try k=5, 3, etc.)\n",
    "- Try adjusting similarity threshold of when model thinks things are similar\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Study query embeddings and potentiall add preprocessing\n",
    "- Further preprocessing of input documents\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5994754,
     "sourceId": 9784510,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
